{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='heading'>\n",
    "    <div style='float:left;'><h1>CPSC 4300/6300: Applied Data Science</h1></div>\n",
    "    <img style=\"float: right; padding-right: 10px; width: 65px\" src=\"https://bsethwalker.github.io/assets/img/clemson_paw.png\">\n",
    "</div>\n",
    "\n",
    "## Course Project (Online Workers) - Checkpoint 1\n",
    "\n",
    "**Clemson University**<br>\n",
    "**Fall 2023**<br>\n",
    "**Instructor(s):** Nina Hubig <br>\n",
    "**Project Team:**\n",
    "<ul>\n",
    "    <li>David Croft <dcroft@g.clemson.edu></li>\n",
    "    <li>Stephen Becker <sgbecke@g.clemson.edu></li>\n",
    "    <li>Tony Hang <qhang@g.clemson.edu></li>\n",
    "    <li>Zachary Trabookis <ztraboo@clemson.edu></li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "\n",
       "div.heading {\n",
       "margin-bottom: 25px;\n",
       "height: 75px;\n",
       "}\n",
       "\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "    \n",
       "    background: rgba(245, 102, 0, .75);\n",
       "    border-color: #E9967A;\n",
       "    border-left: 5px solid #522D80; \n",
       "    padding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "    background-color: #fce8e8;\n",
       "    border-color: #E9967A; \t\n",
       "    border-left: 5px solid #800080; \n",
       "    padding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "    font-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "    background-color: #DDDDDD;\n",
       "    border-color: #E9967A; \t\n",
       "    border-left: 5px solid #800080; \n",
       "    padding: 0.5em;\n",
       "    font-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "    background-color: #AEDE94;\n",
       "    border-color: #E9967A; \t \n",
       "    border-left: 5px solid #800080; \n",
       "    padding: 0.5em;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://bsethwalker.github.io/assets/css/cpsc6300.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Goals\n",
    "\n",
    "* Summary of the data set that, at a minimum, answers the following questions: What is the unit of analysis? How many observations in total are in the data set? How many unique observations are in the data set? What time period is covered?\n",
    "  \n",
    "* Brief summary of any data cleaning steps you have performed. For example, are there any particular observations / time periods / groups / etc. you have excluded?\n",
    "  \n",
    "* Description of outcome with an appropriate visualization technique.\n",
    "  \n",
    "* Description of key predictors with appropriate visualization techniques that compare predictors to the response. You should investigate all predictors in your data as part of your project. For the purpose of this assignment, pick the one or two predictors that you think are going to be most important in explaining the outcome. Your selection of predictors can either be guided by your domain knowledge or be the result of your EDA on all predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Set the max columns to none. This allows all the columns to display for the dataframes.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "# Generic functions for cleaning the data\n",
    "def convert_epoch_time_to_datetime(epoch_time):\n",
    "    \"\"\"\n",
    "    Takes an epoch timestamp and converts it to datetime format\n",
    "    Ref: \n",
    "    https://www.pythonforbeginners.com/basics/convert-epoch-to-datetime-in-python\n",
    "    https://stackoverflow.com/questions/49710963/converting-13-digit-unixtime-in-ms-to-timestamp-in-python \n",
    "    \"\"\"\n",
    "\n",
    "    converted_date = None\n",
    "    try:\n",
    "        # Divide by 1,000 to remove ms time\n",
    "        converted_date = datetime.fromtimestamp(int(epoch_time)/1000).isoformat()\n",
    "    except ValueError as err:\n",
    "        # print(f\"Cannot convert epoch time {epoch_time} to isodate {err}\")\n",
    "\n",
    "        try:\n",
    "            date.fromisoformat(str(epoch_time))\n",
    "        except ValueError as err:\n",
    "            # Value passed is already in the correct `iso` format. Nothing else to do here.\n",
    "            return epoch_time\n",
    "    \n",
    "    return converted_date\n",
    "\n",
    "# Testing the epoch time conversion\n",
    "# convert_epoch_time_to_datetime(1588990000000) # '2020-05-08T22:06:40'\n",
    "# convert_epoch_time_to_datetime(1588994215395)   # '2020-05-08T23:16:55.395000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "import flatdict\n",
    "\n",
    "from collections.abc import MutableMapping\n",
    "\n",
    "def flatten_json_column(json_cols):\n",
    "    \"\"\"\n",
    "    This function flattens JSON columns to individual columns\n",
    "    It merges the flattened dataframe with expected dataframe to capture missing columns from JSON\n",
    "    :param df: Crowd Work Data CSV raw dataframe\n",
    "    :param json_cols: custom data columns in CSV's\n",
    "    :param custom_df: expected dataframe\n",
    "    :return: returns df pandas dataframe\n",
    "\n",
    "    Ref: \n",
    "    https://github.com/vvgsrk/ParseCSVContainsJSONUsingPandas/tree/main\n",
    "    https://avithekkc.medium.com/how-to-convert-nested-json-into-a-pandas-dataframe-9e8779914a24\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure to sort the `na_positions` last because this could effect how many columns\n",
    "    # that the nested column values are shown. If the nested column value is `NaN` first then\n",
    "    # nothing will get populated for those nested column fields. (e.g. `c4_project.hit_requirements`)\n",
    "    # Note: Comment out the fields that you don't want to show up in the final dataframe.\n",
    "    struct_data_json = {\n",
    "        \"task_id\": [None],\n",
    "        \"assignment_id\": [None],\n",
    "        \"accepted_at\": [None],\n",
    "        \"deadline\": [None],\n",
    "        \"time_to_deadline_in_seconds\": [None],\n",
    "        \"state\": [None],\n",
    "        \"question.value\": [None],\n",
    "        \"question.type\": [None],\n",
    "        # \"question.attributes\": [None],\n",
    "        \"question.attributes.FrameSourceAttribute\": [None],\n",
    "        \"question.attributes.FrameHeight\": [None],\n",
    "        \"project.hit_set_id\": [None],\n",
    "        \"project.title\": [None],\n",
    "        \"project.requester_id\": [None],\n",
    "        \"project.requester_name\": [None],\n",
    "        \"project.description\": [None],\n",
    "        \"project.assignment_duration_in_seconds\": [None],\n",
    "        \"project.creation_time\": [None],\n",
    "        \"project.assignable_hits_count\": [None],\n",
    "        \"project.latest_expiration_time\": [None],\n",
    "        \"project.caller_meets_requirements\": [None],\n",
    "        \"project.caller_meets_preview_requirements\": [None],\n",
    "        \"project.last_updated_time\": [None],\n",
    "        \"project.monetary_reward.currency_code\": [None],\n",
    "        \"project.monetary_reward.amount_in_dollars\": [None],\n",
    "        # \"project.hit_requirements.qualification_type_id\": [None],\n",
    "        # \"project.hit_requirements.comparator\": [None],\n",
    "        # \"project.hit_requirements.worker_action\": [None],\n",
    "        # \"project.hit_requirements.qualification_values\": [None],\n",
    "        # \"project.hit_requirements.caller_meets_requirement\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.qualification_type_id\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.name\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.visibility\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.description\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.has_test\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.is_requestable\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.keywords\": [None],\n",
    "        # \"project.hit_requirements.caller_qualification_value.integer_value\": [None],\n",
    "        # \"project.hit_requirements.caller_qualification_value.locale_value.country\": [None],\n",
    "        # \"project.hit_requirements.caller_qualification_value.locale_subdivision\": [None],\n",
    "        \"project.requester_url\": [None],\n",
    "        \"expired_task_action_url\": [None],\n",
    "        \"task_url\": [None]\n",
    "    }\n",
    "\n",
    "    def _flatten_dict(d: MutableMapping, sep: str= '.') -> MutableMapping:\n",
    "        \"\"\"\n",
    "        Take in \n",
    "        \"\"\"\n",
    "        [flat_dict] = pd.json_normalize(data=d, sep=sep, max_level=None).to_dict(orient='records')\n",
    "        return flat_dict\n",
    "\n",
    "    try:\n",
    "        df_temp = pd.DataFrame(struct_data_json)\n",
    "\n",
    "        # If c4 `nan` value is passed, do nothing except return empty dataframe.\n",
    "        # If c4 has a string dicionary, then build new dataframe from it.\n",
    "        if isinstance(json_cols, str):\n",
    "            # Convert the input (str) to (dict) type.\n",
    "            # Build a flattened dictionary before sending to Pandas to `json_normalize`\n",
    "            dict_json_flattened = _flatten_dict(json.loads(json_cols))\n",
    "\n",
    "            # Explicitly remove this column because it's a nested list and is hard to flatten.\n",
    "            # Plus this column doesn't have any values that we need for our model.\n",
    "            del dict_json_flattened[\"project.hit_requirements\"]\n",
    "\n",
    "            df_temp = pd.DataFrame([dict_json_flattened])\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"flatten_json_columns: Invalid JSON argument passed - json.JSONDecodeError: {e}\")\n",
    "\n",
    "    # Return dataframe with flatten columns and 'c4.' prefix.\n",
    "    return df_temp.add_prefix('c4.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning: Reading in the telemetry data (Amazon Mechanical Turk (AMT))\n",
    "We read in and clean the data from `amazon_mechanical_turk_records.csv`.\n",
    "\n",
    "#### Background Research Paper\n",
    "Research Paper: *Quantifying the Invisible Labor in Crowd Work*\n",
    "\n",
    "https://dl.acm.org/doi/abs/10.1145/3476060?casa_token%3Dw4mZH0IjVgsAAAAA:XBgWg_Oq0TtNVqH8SzCxl2fXU_fZ9bzQ6g22QkI0odMy5NKW2EJdYrOaqxu_2NIqJs-rA_sM1sbT\n",
    "\n",
    "A browser plugin was used to collect the data. \n",
    "- https://github.com/GigPlatform/toloka-web-extension (has not been used for any analysis on any paper)\n",
    "- https://github.com/anonym-research/invisible-labor (was used for an invisible labor analysis but any predictive analysis has been performed)\n",
    "\n",
    "**List of available variables:** \n",
    "\n",
    "- **c1 (ID)**: continuous\n",
    "- **c2 (current)**: url, site visited by the worker (while working)\n",
    "- **c3 (event)**: categorical, 18 values (['PAGE_LOAD', 'PAGE_BLUR', 'TAB_CHANGE', 'PAGE_FOCUS', 'PAGE_CLICK', 'PAGE_SCROLL', 'PAGE_LAST', 'PAGE_CLOSE', 'INTERNALURL', 'PAGE_KEY', 'PAGE_INACTIVITY', 'TAB_CLOSED', 'EXTERNALURL', 'PAGE_REACTIVATE', 'SYSTEM_DISABLED_WORKING', 'SYSTEM_ENABLED_WORKING', 'SYSTEM_ENABLED', 'SYSTEM_DISABLED'])\n",
    "  - The web browser plugin recorded multiple events, the most relevant is PAGE_LOAD, other events can provide repetitive information.\n",
    "- **c4 (extra)**: json object {task_id, assignment_id, ...} – may include NaN values\n",
    "  - It provides a JSON object with the specificities of the tasks, it only has values for certain events.\n",
    "  - Are there any values out of this list that we should pay particular attention to?\n",
    "    - Yes, I recommend parsing the JSON so you can find information about the task, including how much was paid.\n",
    "- **c5 (platform)**: categorial, 5 values ['OTHER', 'MTURK', 'FIVERR', 'UPWORK', 'FREELANCER']\n",
    "  - The work platform in which the worker was working on (it is usually constant)\n",
    "- **c6 (skip)**: categorial, 2 values (0: no complete, 1: complete) – may include NaN values\n",
    "  - That was a field that was not used\n",
    "  - You mentioned skip. Does this represent if the task was completed or skipped?\n",
    "    - I do not remember the purpose of that field, maybe was not used\n",
    "- **c7 (subtype)**: categorial, 29 values ['OTHER', 'TASK_STARTED', 'ADDED_TASK', 'TASK_SUBMITED', 'FINISHED_TASK', 'TASKS_LIST', 'WORKER_DASHBOARD', 'UNKNOWN', 'TASK_FRAME', 'TASK_PREVIEW', 'TASK_INFO', 'TASK_RETURNED', 'PLATFORM_LOGIN', 'TASK_QUEUE', 'TASK_SKIP', 'WORKER_EARNINGS_DETAILS', 'TASK_TIMEOUT', 'WORKER_EARNINGS', 'WORKER_QUALIFICATIONS', 'TASKS_PER_REQUESTER', 'MESSAGES_SEND', 'TASKS_LIST_FILTER', 'WORKER_QUALIFICATIONS_PENDING', 'TASKS_PREVIEW', 'PLATFORM_HELP', 'TASKS_PROJECTS', 'TASKS_DETAILS', 'MESSAGES_READ', 'TASKS_APPLY']\n",
    "  - It defines if a worker is listing the tasks available, if a task just started, or if a task was completed (submitted)\n",
    "  - Is there a description that we can lookup to find what these event values mean?\n",
    "    - FINISHED_TASK == TASK_SUBMITED both refers to task completed (submitted is when it was recently submitted and finished when the next URL was loaded)\n",
    "    - TASK_RETURNED When the worker decided not to work on a task\n",
    "- **c8 (time)**: continuous, datetime (milliseconds), 1970 start date Unix Time (Week, Month, Day, Hours, Minutes, Seconds)\n",
    "  - This is a timestamp in milliseconds. You have to convert to a date, it contains day, month, year, hour, minute, second.\n",
    "  - You can use any function that converts from timestamp to datetime.\n",
    "  - What does this time represent? Task completed?\n",
    "    - Time of the event, remember that this is an event log, every event happened at this time. Time series analysis is a common approach to use.\n",
    "  - Does this time represent when the worker did the event recorded in c9 (type)?\n",
    "    - Worker was working \n",
    "    - Communicating by sending messages in the platform \n",
    "    - Searching for tasks\n",
    "    - Visiting their profile\n",
    "    - That is correct. it is the time at which it was recorded\n",
    "- **c9 (type)**: categorial, 10 values ['OTHER', 'WORKING', 'LOGS', 'SEARCHING', 'PROFILE', 'UNKNOWN', 'REJECTED', 'COMMUNICATION', 'LEARNING', 'PROPOSAL']\n",
    "  - It identify if at that time the worker was working, communicating by sending messages in the platform, searching for tasks, visiting their profile. There are other types that mean that the workers changed the CONFIG of the web plugin or the API of Toloka retrieved new tasks.\n",
    "  - Do you have a description for all these events?\n",
    "    - I do not have a data dictionary but these are separated by the events from the worker in the interface: worker was working, communicating by sending messages in the platform, searching for tasks, visiting their profile. The other events do not represent an activity of the worker but a state of the plugin, does not mean any worker activity these only got extra information from the web plugin.\n",
    "- **c10 (user)**: categorical, 120 values unique – Todo: Need to verify this is correct field value.\n",
    "  - User ID\n",
    "- **c11**: Not relevant (an activity was taken after being recommended)\n",
    "- **c12**: Not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_user_to_csv(user_id):\n",
    "    \"\"\"\n",
    "    Read in the cleaned Amazon MT dataset and write it out for a particular user (e.g `ae862298385abab2a0a1619f8cedef9d`)\n",
    "    Convert the `c4` event column by flattening most dict values into separate columns and \n",
    "    write out to temporary *.csv to run limited records moving forward.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the data into a dataframe\n",
    "\n",
    "    # Suggest writing this transformed data out to a file to read in that transformed file for further processing.\n",
    "    df_temp_user = pd.read_csv(f\"../data/amazon_mechanical_turk_records_{user_id}.csv\", encoding='utf-8', header=\"infer\")\n",
    "\n",
    "    df_c4_flattened = pd.DataFrame()\n",
    "    for i, j in df_temp_user.iterrows():\n",
    "        if j[\"c4\"]:\n",
    "            # https://stackoverflow.com/questions/33094056/is-it-possible-to-append-series-to-rows-of-dataframe-without-making-a-list-first\n",
    "            series_temp = flatten_json_column(j[\"c4\"]).iloc[0].to_frame().T\n",
    "            df_c4_flattened = pd.concat([df_c4_flattened, series_temp], ignore_index=True)\n",
    "            # if i == 100:  \n",
    "            #     break    \n",
    "\n",
    "    df_temp_user = pd.concat([df_temp_user, df_c4_flattened], axis=\"columns\")\n",
    "    del df_temp_user[\"c4\"]\n",
    "\n",
    "    df_temp_user.to_csv(f\"../data/amazon_mechanical_turk_records_{user_id}_flattened.csv\",\n",
    "                encoding='utf-8', header=True, index=False, mode=\"w\")\n",
    "    # df_amt.to_csv(f\"../data/amazon_mechanical_turk_records_flattened.csv\",\n",
    "    #             encoding='utf-8', header=True, index=False, mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_users_to_csvs():\n",
    "    \"\"\"\n",
    "    Read in the original Amazon MT dataset and write it out for a particular user (e.g. `ae862298385abab2a0a1619f8cedef9d`)\n",
    "    Convert the `c8` time column to epoch time and write out to temporary *.csv to run limited records moving forward.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    # Define columns for data\n",
    "    columns = ['c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'time', 'c9', 'user']\n",
    "\n",
    "    # Read the data into a dataframe\n",
    "\n",
    "    # 4m 18.8s – Suggest writing this transformed data out to a file to read in that transformed file for further processing.\n",
    "    df_default = pd.read_csv(\"../data/amazon_mechanical_turk_records.csv\", encoding='utf-8', header=None, names=columns, low_memory=False)\n",
    "    \n",
    "    # For now grab only the first 3 users.\n",
    "    # Todo: Need to remove the '[:3]' to grab all users.\n",
    "    unique_users = df_default['user'].dropna().unique().tolist()[:3]\n",
    "\n",
    "    # Write out all flatten files per user.\n",
    "    # for user_id in unique_users:\n",
    "    #     df_temp_user = pd.DataFrame()\n",
    "    #     df_temp_user = df_default[df_default.user == user_id].copy(deep=True)\n",
    "\n",
    "    #     # Convert the epoch timestamp to datetime\n",
    "    #     df_temp_user['time'] = df_temp_user.time.map(convert_epoch_time_to_datetime)\n",
    "\n",
    "    #     df_temp_user.to_csv(f\"../data/amazon_mechanical_turk_records_{user_id}.csv\",\n",
    "    #                 encoding='utf-8', header=True, columns=columns, index=False, mode=\"w\")\n",
    "        \n",
    "    #     flatten_user_to_csv(user_id)\n",
    "\n",
    "    #     os.remove(f\"../data/amazon_mechanical_turk_records_{user_id}.csv\")\n",
    "\n",
    "    \n",
    "    # try:\n",
    "    #     open(\"../data/amazon_mechanical_turk_records_flattened.csv\", 'w').close()\n",
    "    # except FileNotFoundError:\n",
    "    #     # Do nothing if the file doesn't exist.\n",
    "    #     pass\n",
    "\n",
    "    # Write out all flatten files per user into one file.\n",
    "    \n",
    "    # Combine all flattened user files into one.\n",
    "    df_flatten_users = pd.concat(\n",
    "        [\n",
    "            pd.read_csv(\n",
    "                f\"../data/amazon_mechanical_turk_records_{user_id}_flattened.csv\", encoding='utf-8', header=\"infer\"\n",
    "                ) for user_id in unique_users\n",
    "        ], ignore_index=True\n",
    "    )\n",
    "\n",
    "    df_flatten_users.sort_values(by=['user', 'time'], ascending=[True, True]).to_csv(f\"../data/amazon_mechanical_turk_records_users_flattened.csv\",\n",
    "                encoding='utf-8', header=True, index=False, mode=\"w\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this function to create a separate *.csv for a particular user and use that moving forward.\n",
    "# Because this takes long to create the flatten files and combine them we only want to read the \n",
    "# original data we're just calling this once.\n",
    "# 6m 19.8s - first 3 users\n",
    "transform_users_to_csvs()\n",
    "\n",
    "# Read in flattened c4 csv for the user.\n",
    "# df_amt = pd.read_csv(f\"../data/amazon_mechanical_turk_records_{user_id}_flattened.csv\", encoding='utf-8', header=\"infer\")\n",
    "df_amt = pd.read_csv(f\"../data/amazon_mechanical_turk_records_users_flattened.csv\", encoding='utf-8', header=\"infer\")\n",
    "\n",
    "# Sort by user then by time\n",
    "# df_amt = df_amt.sort_values(by=['user', 'time'], ascending=[True, True])\n",
    "\n",
    "# df_amt.head(30)\n",
    "# df_amt[df_amt[\"c4.task_id\"].notna()]\n",
    "# df_amt.iloc[0:16,:]\n",
    "# df_amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54592 entries, 0 to 54591\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                        Non-Null Count  Dtype  \n",
      "---  ------                                        --------------  -----  \n",
      " 0   c1                                            54592 non-null  int64  \n",
      " 1   c2                                            54592 non-null  object \n",
      " 2   c3                                            54592 non-null  object \n",
      " 3   c5                                            54592 non-null  object \n",
      " 4   c6                                            54592 non-null  float64\n",
      " 5   c7                                            54592 non-null  object \n",
      " 6   time                                          54592 non-null  object \n",
      " 7   c9                                            54592 non-null  object \n",
      " 8   user                                          54592 non-null  object \n",
      " 9   c4.task_id                                    866 non-null    object \n",
      " 10  c4.assignment_id                              866 non-null    object \n",
      " 11  c4.accepted_at                                866 non-null    object \n",
      " 12  c4.deadline                                   866 non-null    object \n",
      " 13  c4.time_to_deadline_in_seconds                866 non-null    float64\n",
      " 14  c4.state                                      866 non-null    object \n",
      " 15  c4.question.value                             866 non-null    object \n",
      " 16  c4.question.type                              866 non-null    object \n",
      " 17  c4.question.attributes.FrameSourceAttribute   866 non-null    object \n",
      " 18  c4.question.attributes.FrameHeight            866 non-null    float64\n",
      " 19  c4.project.hit_set_id                         866 non-null    object \n",
      " 20  c4.project.title                              866 non-null    object \n",
      " 21  c4.project.requester_id                       866 non-null    object \n",
      " 22  c4.project.requester_name                     866 non-null    object \n",
      " 23  c4.project.description                        866 non-null    object \n",
      " 24  c4.project.assignment_duration_in_seconds     866 non-null    float64\n",
      " 25  c4.project.creation_time                      866 non-null    object \n",
      " 26  c4.project.assignable_hits_count              866 non-null    float64\n",
      " 27  c4.project.latest_expiration_time             866 non-null    object \n",
      " 28  c4.project.caller_meets_requirements          866 non-null    object \n",
      " 29  c4.project.caller_meets_preview_requirements  866 non-null    object \n",
      " 30  c4.project.last_updated_time                  866 non-null    object \n",
      " 31  c4.project.monetary_reward.currency_code      866 non-null    object \n",
      " 32  c4.project.monetary_reward.amount_in_dollars  866 non-null    float64\n",
      " 33  c4.project.requester_url                      866 non-null    object \n",
      " 34  c4.expired_task_action_url                    866 non-null    object \n",
      " 35  c4.task_url                                   866 non-null    object \n",
      "dtypes: float64(6), int64(1), object(29)\n",
      "memory usage: 15.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Look at the features\n",
    "df_amt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_user_task_start(\n",
    "        df: pd.DataFrame,\n",
    "        user_id: str=\"\",\n",
    "        task_id: str=\"\",\n",
    "        task_started_pattern: str=\"https://worker.mturk.com/projects/(.*)/tasks/(.*)?assignment_id(.*)\"\n",
    "        ) -> (int, datetime):\n",
    "    \"\"\"\n",
    "    Search `c2` url and locate the first event where `c7 (subtype) == TASK_STARTED` and matches\n",
    "    the argument `task_id`.\n",
    "\n",
    "    This limits searching the whole dataset to find the first `c2` url where the task_id has \n",
    "    'TASK_STARTED' generated. Filtering data by c3 == ('PAGE_LOAD', 'TAB_CHANGE') and c7 == 'TASK_STARTED'\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk (need columns c2 (url) and c8 (time))\n",
    "    user_id (str): c10 (user) value.\n",
    "    task_id (str): c4.task_id value.\n",
    "    task_started_pattern (str): Regular expression pattern to identify 'TASK_STARTED'. Defaults to expression in the Chome plugin for AWS MTurk.\n",
    "\n",
    "    Returns:\n",
    "    id: c1 (id) location of dataset record.\n",
    "    datetime: Task id start date for first event 'TASK_STARTED'\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    import re\n",
    "\n",
    "    c1_id = None\n",
    "    c8_event_date = None\n",
    "\n",
    "    # Limit the amount of records needing searched.\n",
    "    df_task_started = df[\n",
    "        (df.user == user_id) & (\n",
    "            ((df.c7 == 'ADDED_TASK') & (df.c9 == 'LOGS')) |\n",
    "            ((df.c7 == 'TASK_STARTED') & ((df.c3 == 'PAGE_LOAD') | (df.c3 == 'TAB_CHANGE')))\n",
    "        )\n",
    "        ]\n",
    "\n",
    "    for i, j in df_task_started.c2.items():\n",
    "        if re.search(task_started_pattern, j, re.IGNORECASE):\n",
    "            # print (i, j)\n",
    "            url_task_id = j.split('/')[6].split('?')[0]\n",
    "            # print(url_task_id)\n",
    "            if url_task_id and (url_task_id == task_id):\n",
    "                try:\n",
    "                    c1_id = df_task_started.c1[i]\n",
    "                    c8_event_date = datetime.datetime.fromisoformat(df_task_started.time[i])\n",
    "                    # print(f\"Found task {task_id} first 'TASK_STARTED': id {c1_id}, date {df_task_started.time[i]}\")    \n",
    "                except ValueError:\n",
    "                    print(f\"Could not find 'TASK_STARTED' event date for task_id {task_id}\")\n",
    "                \n",
    "                # Make sure that we break here to ensure that we're just looking at the \n",
    "                # first 'TASK_STARTED' event time.\n",
    "                break\n",
    "\n",
    "    # We may have a situation where there is no 'TASK_STARTED' for the task but a 'ADDED_TASK' exists.\n",
    "    if not (c1_id and c8_event_date):\n",
    "        try:\n",
    "            # Limit the amount of records needing searched.\n",
    "            df_task_started = df[\n",
    "                (df.user == user_id) &\n",
    "                (df.c7 == 'ADDED_TASK') & \n",
    "                (df.c9 == 'LOGS') & \n",
    "                (df[\"c4.task_id\"] == task_id)\n",
    "                ].iloc[0]\n",
    "        \n",
    "            c1_id = df_task_started.c1\n",
    "            c8_event_date = datetime.datetime.fromisoformat(df_task_started.time)\n",
    "            # print(f\"Found task {task_id} first 'ADDED_TASK': id {c1_id}, date {df_task_started.time}\")    \n",
    "        except (IndexError, ValueError) as err:\n",
    "            print(f\"Could not find 'ADDED_TASK' event date for task_id {task_id}: {err}\")\n",
    "    else:\n",
    "        try:\n",
    "            # Check to see if we have an 'ADDED_TASK' before this 'c1_id' and use it instead if there.\n",
    "            # This is because the background process logs this event every 30 minutes and it does not\n",
    "            # abide by the event order.\n",
    "            # (e.g. 3E9ZFLPWO0KJDJRNXDHG072XWOZXIK)\n",
    "            df_added_task = df_task_started[(df_task_started.c7 == 'ADDED_TASK') & (df_task_started[\"c4.task_id\"] == task_id)].iloc[0]\n",
    "            c1_id = df_added_task.c1\n",
    "            c8_event_date = datetime.datetime.fromisoformat(df_added_task.time)\n",
    "        except IndexError as err:\n",
    "            # No previous 'ADDED_TASK' found, so we'll stick with what we found with 'TASK_STARTED'.\n",
    "            pass\n",
    "\n",
    "    return c1_id, c8_event_date\n",
    "\n",
    "\n",
    "def locate_user_task_end(\n",
    "        df: pd.DataFrame,\n",
    "        user_id: str=\"\",\n",
    "        task_id: str=\"\",\n",
    "        ) -> (int, datetime):\n",
    "    \"\"\"\n",
    "    Search for observations where last event is`c7 (subtype) == FINISHED_TASK` and matches\n",
    "    the argument `task_id`. \n",
    "\n",
    "    Note: We could have used `TASK_SUBMITTED`, however, we noticed some invisible tasks came after.\n",
    "    Carlos mentions the following:\n",
    "    \"FINISHED_TASK == TASK_SUBMITED both refers to task completed (submitted is when it was \n",
    "    recently submitted and finished when the next URL was loaded)\"\n",
    "\n",
    "    This limits searching the whole dataset to find the first `c7` subtype where the task_id has \n",
    "    'FINISHED_TASK' generated. Filtering data by c7 == 'FINISHED_TASK' and c9 (type) == 'LOGS'\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk (need columns c7 (subtype) and c9 (type))\n",
    "    user_id (str): c10 (user) value.\n",
    "    task_id (str): c4.task_id value.\n",
    "\n",
    "    Returns:\n",
    "    id: c1 (id) location of dataset record.\n",
    "    datetime: Task id end date for event 'FINISHED_TASK'\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "\n",
    "    c1_id = None\n",
    "    c8_event_date = None\n",
    "\n",
    "    try:\n",
    "        # Limit the amount of records needing searched to the first row.\n",
    "        df_task_finished = df[\n",
    "            (df.user == user_id) &\n",
    "            (df.c7 == 'FINISHED_TASK') & \n",
    "            (df.c9 == 'LOGS') &\n",
    "            (df[\"c4.task_id\"] == task_id)\n",
    "            ].iloc[0]\n",
    "\n",
    "        c1_id = df_task_finished.c1\n",
    "        c8_event_date = datetime.datetime.fromisoformat(df_task_finished.time)\n",
    "        # print(f\"Found task {task_id} first 'FINISHED_TASK': id {c1_id}, date {df_task_finished.time}\")    \n",
    "    except (IndexError, ValueError) as err:\n",
    "        print(f\"Could not find 'FINISHED_TASK' event date for task_id {task_id}: {err}\")\n",
    "    \n",
    "    return c1_id, c8_event_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 'ADDED_TASK' event date for task_id 302U8RURJY0UZS7SBXEJSBSUBN2VNK: single positional indexer is out-of-bounds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def task_requester_name(df: pd.DataFrame, c1_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Returns project requester name for the 'FINISHED_TASK' event.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk (need column c1 (id) for lookup)\n",
    "    c1_id: The c1 (id) value for lookup int the dataframe passed.\n",
    "\n",
    "    Returns:\n",
    "    int: Seconds of task total time duration. This includes labor (working + invisible).\n",
    "    \"\"\"\n",
    "    requester_name = \"\"\n",
    "    try:\n",
    "        requester_name = df[df.c1 == c1_id].iloc[0][\"c4.project.requester_name\"]\n",
    "    except TypeError as err:\n",
    "        print(f\"Cannot locate project requester name {err}\")\n",
    "\n",
    "    return requester_name\n",
    "\n",
    "\n",
    "def task_estimate_duration_in_seconds(df: pd.DataFrame, c1_id: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns 'c4.project.assignment_duration_in_seconds' duration in seconds from 'FINISHED_TASK' event.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk (need column c1 (id) for lookup)\n",
    "    c1_id: The c1 (id) value for lookup int the dataframe passed.\n",
    "\n",
    "    Returns:\n",
    "    int: Seconds of task total time estimate duration. This includes labor (working + invisible).\n",
    "    \"\"\"\n",
    "    duration = 0\n",
    "    try:\n",
    "        duration = int(df[df.c1 == c1_id].iloc[0][\"c4.project.assignment_duration_in_seconds\"])\n",
    "    except TypeError as err:\n",
    "        print(f\"Cannot calculate task total time duration {err}\")\n",
    "\n",
    "    return duration\n",
    "\n",
    "\n",
    "def task_monetary_reward_in_dollars(df: pd.DataFrame, c1_id: int) -> float:\n",
    "    \"\"\"\n",
    "    Returns 'c4.project.monetary_reward.amount_in_dollars' duration in seconds from 'FINISHED_TASK' event.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk (need column c1 (id) for lookup)\n",
    "    c1_id: The c1 (id) value for lookup int the dataframe passed.\n",
    "\n",
    "    Returns:\n",
    "    int: US dollar amount for the task. This includes labor (working + invisible).\n",
    "    \"\"\"\n",
    "    monetary = 0\n",
    "    try:\n",
    "        monetary = float(df[df.c1 == c1_id].iloc[0][\"c4.project.monetary_reward.amount_in_dollars\"])\n",
    "    except TypeError as err:\n",
    "        print(f\"Cannot calculate task monetary reward {err}\")\n",
    "\n",
    "    return monetary\n",
    "\n",
    "\n",
    "def total_labor_event_count(df: pd.DataFrame, c1_id_start: int, c1_id_end: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns count of events between and including 'TASK_STARTED' to 'FINISHED_TASK' for a task.\n",
    "    (includes working + invisible tasks) \n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk\n",
    "    c1_id_start (int): The task start c1 (id) for 'TASK_STARTED' event time.\n",
    "    c1_id_end (int): The task start c1 (id) for 'FINISHED_TASK' event time.\n",
    "\n",
    "    Returns:\n",
    "    int: Count of task between c1_id_start and c1_id_end (includes working + invisible tasks)\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    try:\n",
    "        count = len(df[df.c1.between(c1_id_start, c1_id_end, inclusive=\"both\") == True])\n",
    "    except TypeError as err:\n",
    "        print(f\"Cannot calculate task count for labor events {err}\")\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def total_labor_duration_in_seconds(dt_start: datetime, dt_end: datetime) -> int:\n",
    "    \"\"\"\n",
    "    Returns duration in seconds between task 'FINISHED_TASK' - 'TASK_STARTED' time.\n",
    "\n",
    "    Parameters:\n",
    "    dt_start (datetime): The task start 'TASK_STARTED' time\n",
    "    dt_end (datetime): The task end 'FINISHED_TASK' time\n",
    "\n",
    "    Returns:\n",
    "    int: Seconds of total time duration.\n",
    "    \"\"\"\n",
    "    duration = 0\n",
    "    try:\n",
    "        duration = (dt_end - dt_start).seconds\n",
    "    except TypeError as err:\n",
    "        print(f\"Cannot calculate labor total time seconds {err}\")\n",
    "\n",
    "    return duration\n",
    "\n",
    "\n",
    "def working_labor_event_count(df: pd.DataFrame, c1_id_start: int, c1_id_end: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns count of 'WORKING' events between and including 'TASK_STARTED' to 'FINISHED_TASK' for a task.\n",
    "    (includes working tasks only)\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk\n",
    "    c1_id_start (int): The task start c1 (id) for 'TASK_STARTED' event time.\n",
    "    c1_id_end (int): The task start c1 (id) for 'FINISHED_TASK' event time.\n",
    "\n",
    "    Returns:\n",
    "    int: Count of task events between c1_id_start and c1_id_end (includes working tasks)\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    try:\n",
    "        # Make sure to sort_values first then reset_index. Otherwise the index could be out of order.\n",
    "        df_working = df[\n",
    "            ((df.c1 >= c1_id_start) & (df.c1 <= c1_id_end)) &\n",
    "            ((df.c9 == 'WORKING') | (df.c9 == 'LOGS'))\n",
    "            ].sort_values(by=['c1'], ascending=[True]).reset_index(drop=True)\n",
    "        \n",
    "        count = len(df_working)\n",
    "    except TypeError as err:\n",
    "        print(f\"Cannot calculate task count for labor working events {err}\")\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def working_labor_duration_in_seconds(df: pd.DataFrame, c1_id_start: int, c1_id_end: int, debug=False) -> int:\n",
    "    \"\"\"\n",
    "    Returns duration 'WORKING' events between and including 'TASK_STARTED' to 'FINISHED_TASK' for a task.\n",
    "    (includes working tasks only)\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk\n",
    "    c1_id_start (int): The task start c1 (id) for 'TASK_STARTED' event time.\n",
    "    c1_id_end (int): The task start c1 (id) for 'FINISHED_TASK' event time.\n",
    "\n",
    "    Returns:\n",
    "    int: Seconds of task events between c1_id_start and c1_id_end (includes working tasks)\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "\n",
    "    if debug:\n",
    "        task_id = df[df[\"c1\"] == c1_id_end].iloc[0][\"c4.task_id\"]\n",
    "        print(f\"\\nTask {task_id}: c1_id_start {c1_id_start}, c1_id_end {c1_id_end}\")\n",
    "\n",
    "    duration = 0\n",
    "    try:\n",
    "        # Make sure to sort_values first then reset_index. Otherwise the index could be out of order.\n",
    "        df_working = df[\n",
    "            ((df.c1 >= c1_id_start) & (df.c1 <= c1_id_end))\n",
    "            ].sort_values(by=['c1'], ascending=[True]).reset_index(drop=True)\n",
    "        \n",
    "        for i, j in df_working.iterrows():\n",
    "            if j[\"c9\"] == 'WORKING':\n",
    "                try:\n",
    "                    dt_working = datetime.datetime.fromisoformat(j[\"time\"])\n",
    "                    dt_next_event = datetime.datetime.fromisoformat(df_working.iloc[i + 1][\"time\"])\n",
    "\n",
    "                    if dt_working < dt_next_event:\n",
    "                        duration += (dt_next_event - dt_working).seconds\n",
    "                        if debug:\n",
    "                            print(f\"Difference of id {df_working.iloc[i + 1]['c1']}: ({dt_next_event}) and id {j['c1']}: ({dt_working}) is {(dt_next_event - dt_working).seconds} seconds.\")\n",
    "                    else:\n",
    "                        # Todo: For task_id = \"3M0556243RJC3RXK4Q2QWOJY3G6NF6\" the 'FINISHED_TASK' \n",
    "                        # has earlier datetiem than the 'WORKING' task that comes before it so \n",
    "                        # we need to swap the dates to avoid a negative date and large seconds value.\n",
    "                        duration += (dt_working - dt_next_event).seconds\n",
    "                        if debug:\n",
    "                            print(f\"Difference of id {df_working.iloc[i + 1]['c1']}: ({dt_next_event}) and id {j['c1']}: ({dt_working}) is {(dt_working - dt_next_event).seconds} seconds.\")\n",
    "\n",
    "                except IndexError as err:\n",
    "                    print(f\"Cannot locate next dataframe duration {err}\")\n",
    "            \n",
    "    except TypeError as err:\n",
    "        print(f\"Cannot calculate task duration for labor working events {err}\")\n",
    "\n",
    "    return duration\n",
    "\n",
    "\n",
    "df_features = pd.DataFrame(columns=[\n",
    "    'user.id',\n",
    "    'task.id',\n",
    "    'task.monetary_reward_in_dollars',\n",
    "    'task.requester_name',\n",
    "    'task.estimate_duration_in_seconds',\n",
    "    'total.labor.event_count',\n",
    "    'total.labor.duration_in_seconds',\n",
    "    'working.labor.event_count',\n",
    "    'working.labor.duration_in_seconds',\n",
    "    'invisible.labor.event_count',\n",
    "    'invisible.labor.duration_in_seconds'\n",
    "])\n",
    "\n",
    "# For now grab only the first 3 users.\n",
    "# Todo: Need to remove the '[:3]' to grab all users.\n",
    "unique_users = df_amt['user'].dropna().unique().tolist()[:3]\n",
    "# '149c64b9f9b890bcf32bd2dcf595fd'\n",
    "# '49e7482b6cda157f388c73b3bcc2ebfc'\n",
    "# 'ae862298385abab2a0a1619f8cedef9d'\n",
    "\n",
    "# Enumerate here to get the index (i) value in case we need to pop(i) values from original list\n",
    "# due to invalid values in the dataset provided.\n",
    "for j, user_id in enumerate(unique_users):\n",
    "\n",
    "    df_amt_user = df_amt[df_amt.user == user_id]\n",
    "\n",
    "    # Use `.tolist()` to convert ndarray to Python list.\n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.ndarray.tolist.html\n",
    "    unique_tasks = df_amt_user['c4.task_id'].dropna().unique().tolist() \n",
    "    # [\"3W9XHF7WGLV68SQR2YVUGGPI6QVTK3\"] \n",
    "    # [\"3KA7IJSNW54MTVXHF3TMHNX8OOTPBI\"]\n",
    "    # [\"3E9ZFLPWO0KJDJRNXDHG072XWOZXIK\"]\n",
    "\n",
    "    for i, t in enumerate(unique_tasks):\n",
    "\n",
    "        task_c1_id_start, task_start_date = locate_user_task_start(\n",
    "            df_amt_user, user_id, t, \"https://worker.mturk.com/projects/(.*)/tasks/(.*)?assignment_id(.*)\"\n",
    "            )\n",
    "        task_c1_id_end, task_end_date = locate_user_task_end(\n",
    "            df_amt_user, user_id, t\n",
    "            )\n",
    "        \n",
    "        # If we have record that doesn't have a start time then let's remove it.\n",
    "        # Example: '302U8RURJY0UZS7SBXEJSBSUBN2VNK' doesn't include a 'ADDED_TASK' or 'TASK_STARTED' event.\n",
    "        if not task_c1_id_start:\n",
    "            unique_tasks.pop(i)\n",
    "            continue\n",
    "\n",
    "        # Calculate totals for (t = Total, w = Working, i = Invisible)\n",
    "        t_labor_event_count = total_labor_event_count(df_amt_user, task_c1_id_start, task_c1_id_end)\n",
    "        t_labor_duration_in_seconds = total_labor_duration_in_seconds(task_start_date, task_end_date)\n",
    "        w_labor_event_count = working_labor_event_count(df_amt_user, task_c1_id_start, task_c1_id_end)\n",
    "        w_labor_duration_in_seconds = working_labor_duration_in_seconds(df_amt_user, task_c1_id_start, task_c1_id_end, debug=False)\n",
    "        i_labor_event_count = t_labor_event_count - w_labor_event_count\n",
    "        i_labor_duration_in_seconds = t_labor_duration_in_seconds - w_labor_duration_in_seconds\n",
    "\n",
    "        df_features.loc[len(df_features.index)] = [\n",
    "            user_id,\n",
    "            t,\n",
    "            task_monetary_reward_in_dollars(df_amt_user, task_c1_id_end),\n",
    "            task_requester_name(df_amt_user, task_c1_id_end),\n",
    "            task_estimate_duration_in_seconds(df_amt_user, task_c1_id_end),\n",
    "            t_labor_event_count,\n",
    "            t_labor_duration_in_seconds,\n",
    "            w_labor_event_count,\n",
    "            w_labor_duration_in_seconds,\n",
    "            i_labor_event_count,\n",
    "            i_labor_duration_in_seconds\n",
    "        ]\n",
    "\n",
    "# Write out to the final feature *.csv file.\n",
    "df_features.to_csv(f\"../data/cloudworker_tasks.csv\",\n",
    "                encoding='utf-8', header=True, index=False, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the unit of analysis?\n",
    "\n",
    "## How many observations in total are in the data set? \n",
    "\n",
    "## How many unique observations are in the data set? \n",
    "\n",
    "## What time period is covered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief summary of any data cleaning steps you have performed. For example, are there any particular observations / time periods / groups / etc. you have excluded?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of outcome with an appropriate visualization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of key predictors with appropriate visualization techniques that compare predictors to the response. You should investigate all predictors in your data as part of your project. For the purpose of this assignment, pick the one or two predictors that you think are going to be most important in explaining the outcome. Your selection of predictors can either be guided by your domain knowledge or be the result of your EDA on all predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning: Reading in the telemetry data (Toloka)\n",
    "We read in and clean the data from `toloka_telemetry_db.csv`.\n",
    "\n",
    "**List of available variables (includes target variable `TBD`):**\n",
    "\n",
    "- **c1**: continuous\n",
    "- **c2**: url to work task\n",
    "- **current**: categorial, 31 values ['PAGE_LOAD', 'TAB_CLOSED', 'PAGE_BLUR', 'PAGE_FOCUS', 'TAB_CHANGE', 'CONFIG_UPDATE', 'PLUGIN_INSTALL', 'CONFIG_FILE', 'APP_ACTIVATED', 'PAGE_CLOSE', 'USER', 'BELL_CLICK', 'PAGE_LAST', 'PAGE_CLICK', 'PAGE_KEY', 'PAGE_SCROLL', 'TASK', 'PAGE_INACTIVITY', 'TRAINING', 'PAGE_REACTIVATE', 'LIST_NEW', 'LIST_RECOM', 'LIST_PAY', 'SETT_CLICK', 'MSG_RCV_WORKER', 'MSG_CLICK_WORKER', 'TASK_HIDE_OFF', 'TASK_HIDE_ON', 'MSG_RCV_REQUESTER', 'SETT_SAVE', 'MSG_CLICK_REQUESTER'] - May need to remove/transform urls (e.g. 'https://toloka.yandex.com)\n",
    "- **event**: json object {activeAssignments, ...} - may include NaN values\n",
    "- **platform**: categorial, 2 values {0, NaN}\n",
    "- **subtype**: categorial, 24 values ['TASK_STARTED', 'TASKS_LIST', 'TASK_SUBMITED', 'OTHER', 'FINISHED_TASK', 'SYSTEM', 'GENERAL', 'ADDED_TASK', 'META_DATA', 'UNKNOWN', 'TASK_QUEUE', 'WORKER_QUALIFICATIONS', 'WORKER_DASHBOARD', 'WORKER_EARNINGS', 'WORKER_EARNINGS_DETAILS', 'TASK_INFO', 'MESSAGES_READ', 'TASK_TIMEOUT', 'REFERRAL', 'NOTIFICATIONS', 'MESSAGES_REQUESTER', 'MESSAGES_OUTBOX', 'MESSAGES_ADMIN', 'MESSAGES_NOTIFICATION'] \n",
    "- **time**: continous (duration)\n",
    "- **type**: categorial, 11 values ['WORKING', 'SEARCHING', 'OTHER', 'LOGS', 'CONFIG', 'API', 'SYSTEM', 'UNKNOWN', 'PROFILE', 'COMMUNICATION', 'REJECTED']\n",
    "- **user**: categorial, user id\n",
    "- **ordinal**: continuous, 2 values [1, 2]\n",
    "- **unnamed**: continous, values [nan, 0.00000e+00, 2.89000e+02, ..., 3.79992e+05, 3.25610e+04, 3.61200e+04] – may include NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for data\n",
    "columns = ['c1', 'c2', 'current', 'event', 'extra', 'platform', 'subtype', 'time', 'type', 'user', 'ordinal', 'unnamed']\n",
    "\n",
    "# Read the data into a dataframe (this has a header so remove row 0)\n",
    "df_toloka = pd.read_csv(\"../data/toloka_telemetry_db.csv\", encoding='utf-8', header=0, names=columns)\n",
    "\n",
    "# Convert the epoch timestamp to datetime\n",
    "df_toloka['time']=df_toloka.time.map(convert_epoch_time_to_datetime)\n",
    "\n",
    "# Sort by user then by time\n",
    "df_toloka = df_toloka.sort_values(by=['user', 'time'], ascending=[True, True])\n",
    "\n",
    "#Examine the first few rows of the dataframe\n",
    "df_toloka.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the features\n",
    "df_toloka.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output continuous/categorical columns to see what the data has.\n",
    "\n",
    "# df_toloka.current.unique()\n",
    "# array(['PAGE_LOAD', 'TAB_CLOSED', 'PAGE_BLUR', 'PAGE_FOCUS', 'TAB_CHANGE',\n",
    "#        'CONFIG_UPDATE', 'PLUGIN_INSTALL', 'CONFIG_FILE', 'APP_ACTIVATED',\n",
    "#        'PAGE_CLOSE', 'USER', 'BELL_CLICK', 'PAGE_LAST', 'PAGE_CLICK',\n",
    "#        'PAGE_KEY', 'PAGE_SCROLL', 'TASK', 'PAGE_INACTIVITY', 'TRAINING',\n",
    "#        'PAGE_REACTIVATE', 'LIST_NEW', 'LIST_RECOM', 'LIST_PAY',\n",
    "#        'SETT_CLICK', 'MSG_RCV_WORKER', 'MSG_CLICK_WORKER',\n",
    "#        'https://toloka.yandex.com/tasks', 'TASK_HIDE_OFF', 'TASK_HIDE_ON',\n",
    "#        'MSG_RCV_REQUESTER', 'SETT_SAVE',\n",
    "#        'https://toloka.yandex.com/vi/tasks',\n",
    "#        'https://toloka.yandex.com/fr/messages',\n",
    "#        'https://toloka.yandex.com/es/tasks', 'MSG_CLICK_REQUESTER',\n",
    "#        'https://toloka.yandex.com/profile/history/73643/2022-01-20',\n",
    "#        'https://toloka.yandex.com/fr/tasks',\n",
    "#        'https://toloka.yandex.com/task/31740731?refUuid=75fd7974-884b-4fff-aa42-b08d20fba5b4',\n",
    "#        'https://toloka.yandex.com/task/21133006/',\n",
    "#        'https://toloka.yandex.com/tasks/active',\n",
    "#        'https://toloka.yandex.com/profile/history?status=all',\n",
    "#        'https://toloka.yandex.com/profile/money',\n",
    "#        'https://toloka.yandex.com/messages/inbox/620abf1a68ab666108b4b4a0',\n",
    "#        'https://toloka.yandex.com/task/31161320?refUuid=6acf2a47-f652-4d2a-b1de-4f4c4724c5f6',\n",
    "#        'https://toloka.yandex.com/task/2990538/00002da1ca--620b8c1db0c49a3a992921d4',\n",
    "#        'https://toloka.yandex.com/task/31746103/0001e46837--620bb1bec930da47000174d0',\n",
    "#        'https://toloka.yandex.com/profile/history?status=income',\n",
    "#        'https://toloka.yandex.com/profile/history?status=blocked',\n",
    "#        'https://toloka.yandex.com/profile/edit',\n",
    "# ...\n",
    "#        'https://toloka.yandex.com/ru/tasks',\n",
    "#        'https://toloka.yandex.com/ru/task/32969499/',\n",
    "#        'https://toloka.yandex.com/ru/task/32969361/',\n",
    "#        'https://toloka.yandex.com/ru/task/33037260',\n",
    "#        'https://toloka.yandex.com/ru/messages'], dtype=object)\n",
    "\n",
    "\n",
    "# df_toloka.event.unique()\n",
    "# array([nan,\n",
    "#        \n",
    "#        '{\"browserName\":\"CHROME\",\"currentMode\":\"PASSIVE\",\"currentState\":0,\"dailySurveyUrl\":\"https://docs.google.com/forms/d/e/1FAIpQLSe2XInPGnb9EI539KyUiBNr6gcmRNPzg55LRUaEmXFtx_3zqg/viewform?usp=pp_url&entry.1915115278=\",\"finalSurveyUrl\":\"https://docs.google.com/forms/d/e/1FAIpQLSefhPyHJs9x-_v0WxQ5aI3kGF3hfXNZH2vk3KMx1C8CnoZSGw/viewform?usp=pp_url&entry.1172212682=\",\"groupId\":\"GN\",\"hideUnpaidTasks\":true,\"initialSurveyUrl\":\"https://docs.google.com/forms/d/e/1FAIpQLSdWQa1AkVc5vV0VhdOYQLGgK77Pw-LEpmsuCBKpdo7QQopWbg/viewform?usp=pp_url&entry.784114392=\",\"installTime\":1642708724229,\"instructionsUrl\":\"https://bit.ly/cul-act-gn\",\"isUserStudy\":true,\"logServerUrl\":\"https://script.google.com/macros/s/AKfycbwGx2_5a6IwcNI2YZuz2AZvb1J-7Y8Ulk5fYHjZoA8wvHzajv9P55DYiI8UnoV0W403HA/exec\",\"mode\":\"PROTOCOL\",\"nextDue\":1643313524229,\"pluginName\":\"Toloka Assistant\",\"protocol\":[{\"durationMins\":10080,\"mode\":\"PASSIVE\"},{\"durationMins\":10080,\"mode\":\"ACTIVE\"},{\"durationMins\":10080,\"mode\":\"FINISH\"}],\"rankMethod\":\"AI\",\"sandbox\":false,\"serverUrls\":[\"https://script.google.com/macros/s/AKfycbwGx2_5a6IwcNI2YZuz2AZvb1J-7Y8Ulk5fYHjZoA8wvHzajv9P55DYiI8UnoV0W403HA/exec\",\"https://hcilab.ml/overhead/api\"],\"settings\":{\"msg_requ\":true,\"msg_work\":true,\"not_brow\":true,\"not_page\":true,\"not_whil\":true,\"num_task\":5},\"socketUrl\":\"http://hcilab.ml:5000\",\"studyDurationDays\":14,\"studyDurationMins\":20160,\"userData\":{\"acceptedEula\":12,\"actualUser\":{\"defaultEmail\":\"carlostoxtli@yandex.com\",\"displayName\":\"carlostoxtli\",\"login\":\"carlostoxtli\",\"readOnlyModeToActUnderAccount\":false,\"role\":\"WORKER\",\"uid\":1274303000,\"userLang\":\"ES\"},\"adultAllowed\":true,\"authoritiesInfo\":{\"issuedAuthorities\":[\"APP_USER\",\"U_WALLETS_EDIT\",\"U_ASSIGNMENTS_VIEW\",\"U_TRANSACTIONS_CREATE\",\"U_ASSIGNMENTS_UNDERTAKE\",\"APP\",\"U_ASSIGNMENTS_HISTORY\",\"U_PROFILE_VIEW\",\"U_TRANSACTIONS_VIEW\",\"U_ASSIGNMENTS_SUBMIT\",\"U_FORUM_VIEW\",\"U_MESSAGES_CREATE\",\"U_FORUM_EDIT\",\"U_PROFILE_EDIT\",\"U_MESSAGES_VIEW\"],\"notIssuedAuthoritiesReasons\":{}},\"availableAccounts\":[],\"balance\":0,\"birthDay\":\"1982-11-15\",\"blockedBalance\":0,\"citizenship\":\"US\",\"cityId\":103027,\"country\":\"US\",\"createdDate\":\"2020-12-21\",\"defaultEmail\":\"carlostoxtli@yandex.com\",\"displayName\":\"carlostoxtli\",\"education\":\"HIGH\",\"firstName\":\"Carlos\",\"fullName\":\"Carlos Toxtli\",\"gender\":\"MALE\",\"isAccountOwner\":true,\"languages\":[\"EN\",\"ES\"],\"lastName\":\"Toxtli\",\"login\":\"carlostoxtli\",\"rating\":0,\"regionId\":223,\"role\":\"WORKER\",\"systemBan\":false,\"uid\":1274303000,\"userLang\":\"ES\"},\"userId\":\"a5d84fcd0637d31f4675cdf17b71a35\"}',\n",
    "#        ...,\n",
    "#        '{\"refUuid\":\"4eb5cecf-42ce-4580-b89d-c2edb9624873\",\"groupUuid\":\"2cacd9db-5da7-4ff4-9032-d38402d5825d\",\"lightweightTec\":{\"poolId\":33139449,\"projectId\":58019,\"poolStartedAt\":\"2022-04-25T10:00:27.146\",\"mayContainAdultContent\":true,\"title\":\"Find content from website (universal app prod)\",\"description\":\"Find the exact web page that contains the listed information from the given website domain. Use the google translate or bing translate browser extensions to translate international web pages into understandable language.\\\\\\\\nНайдите точную веб-страницу, содержащую перечисленную информацию из данного домена веб-сайта. Используйте расширения браузера google translate или bing translate, чтобы переводить международные веб-страницы на понятный язык.\\\\\\\\n\",\"hasInstructions\":true,\"snapshotMajorVersion\":1,\"snapshotMinorVersion\":8,\"snapshotMajorVersionActual\":true,\"assignmentConfig\":{\"reward\":\"0.020\",\"maxDurationSeconds\":600,\"issuing\":{\"type\":\"AUTOMATIC\"}},\"trainingConfig\":{\"training\":false},\"requesterInfo\":{\"id\":\"97e0e18092318a1140eb08402e7cc5ac\",\"name\":{\"EN\":\"Bing Local Search 2\",\"FR\":\"Bing Local Search 2\",\"ID\":\"Bing Local Search 2\",\"RU\":\"Bing Local Search 2\",\"TR\":\"Bing Local Search 2\"},\"trusted\":false},\"projectMetaInfo\":{\"projectId\":58019,\"bookmarked\":true,\"bookmarkedAt\":\"2022-03-12T01:28:35.577\",\"experimentMeta\":{\"dj_task_duration__snippet__duration_less_than_minute\":\"1\",\"dj_project_class__snippet__web_searching\":\"1\",\"dj_project_tag__requester_type__snippet__experienced_requester\":\"1\"}},\"iframeSubdomain\":\"97e0e18092318a1140eb08402e7cc5ac\"},\"availability\":{\"available\":true},\"activeAssignments\":[{\"id\":\"0001f9aaf9--6266ca43ca6f212f45a79130\",\"expireTime\":\"2022-04-25T16:30:19.528\",\"secondsLeft\":597,\"reward\":0.02}],\"acceptanceDetails\":{\"postAccept\":true,\"acceptanceRate\":99,\"acceptancePeriodDays\":1,\"averageAcceptancePeriodDays\":1},\"trainingDetails\":{\"training\":false},\"taskDetails\":{\"grade\":{\"total_grade\":4.87},\"averageSubmitTimeSec\":23,\"averageAcceptanceTimeSec\":86406,\"moneyAvgHourly\":3.13043472,\"moneyAvg\":17.48184971098265,\"moneyMed\":18.22,\"moneyTop10\":30.288000000000004,\"moneyMax3\":18.330940090548125},\"grade\":{\"total_grade\":4.87}}',\n",
    "#        '{\"refUuid\":\"e046bcc5-ece8-40b1-8203-e48fed3ddc99\",\"groupUuid\":\"9fde347c-5a79-4e78-9023-fe325f8bd616\",\"lightweightTec\":{\"poolId\":33139449,\"projectId\":58019,\"poolStartedAt\":\"2022-04-25T10:00:27.146\",\"mayContainAdultContent\":true,\"title\":\"Find content from website (universal app prod)\",\"description\":\"Find the exact web page that contains the listed information from the given website domain. Use the google translate or bing translate browser extensions to translate international web pages into understandable language.\\\\\\\\nНайдите точную веб-страницу, содержащую перечисленную информацию из данного домена веб-сайта. Используйте расширения браузера google translate или bing translate, чтобы переводить международные веб-страницы на понятный язык.\\\\\\\\n\",\"hasInstructions\":true,\"snapshotMajorVersion\":1,\"snapshotMinorVersion\":8,\"snapshotMajorVersionActual\":true,\"assignmentConfig\":{\"reward\":\"0.020\",\"maxDurationSeconds\":600,\"issuing\":{\"type\":\"AUTOMATIC\"}},\"trainingConfig\":{\"training\":false},\"requesterInfo\":{\"id\":\"97e0e18092318a1140eb08402e7cc5ac\",\"name\":{\"EN\":\"Bing Local Search 2\",\"FR\":\"Bing Local Search 2\",\"ID\":\"Bing Local Search 2\",\"RU\":\"Bing Local Search 2\",\"TR\":\"Bing Local Search 2\"},\"trusted\":false},\"projectMetaInfo\":{\"projectId\":58019,\"bookmarked\":true,\"bookmarkedAt\":\"2022-03-12T01:28:35.577\",\"experimentMeta\":{\"dj_task_duration__snippet__duration_less_than_minute\":\"1\",\"dj_project_class__snippet__web_searching\":\"1\",\"dj_project_tag__requester_type__snippet__experienced_requester\":\"1\"}},\"iframeSubdomain\":\"97e0e18092318a1140eb08402e7cc5ac\"},\"availability\":{\"available\":true},\"activeAssignments\":[{\"id\":\"0001f9aaf9--6266ca43ca6f212f45a79130\",\"expireTime\":\"2022-04-25T16:30:19.528\",\"secondsLeft\":567,\"reward\":0.02}],\"acceptanceDetails\":{\"postAccept\":true,\"acceptanceRate\":99,\"acceptancePeriodDays\":1,\"averageAcceptancePeriodDays\":1},\"trainingDetails\":{\"training\":false},\"taskDetails\":{\"grade\":{\"total_grade\":4.87},\"averageSubmitTimeSec\":23,\"averageAcceptanceTimeSec\":86406,\"moneyAvgHourly\":3.13043472,\"moneyAvg\":17.48184971098265,\"moneyMed\":18.22,\"moneyTop10\":30.288000000000004,\"moneyMax3\":18.330940090548125},\"grade\":{\"total_grade\":4.87}}',\n",
    "#        '{\"uid\":1206161147,\"login\":\"sholesy@gmail.com\",\"role\":\"WORKER\",\"userLang\":\"EN\",\"defaultEmail\":\"sholesy@gmail.com\",\"connectionId\":\"s:1650784330835:uZlwaQ:2d\",\"authorizationStatus\":\"VALID\",\"avatarId\":\"0/0-0\",\"displayName\":\"sholesy@gmail.com\",\"fullName\":\"Oluwatosin Solesi\",\"firstName\":\"Oluwatosin\",\"lastName\":\"Solesi\",\"isAccountOwner\":true,\"actualUser\":{\"uid\":1206161147,\"login\":\"sholesy@gmail.com\",\"role\":\"WORKER\",\"userLang\":\"EN\",\"defaultEmail\":\"sholesy@gmail.com\",\"displayName\":\"sholesy@gmail.com\",\"readOnlyModeToActUnderAccount\":false},\"availableAccounts\":[],\"createdDate\":\"2020-10-26\",\"systemBan\":false,\"gender\":\"FEMALE\",\"birthDay\":\"1991-10-14\",\"cityId\":21063,\"country\":\"NG\",\"citizenship\":\"US\",\"education\":\"HIGH\",\"languages\":[\"EN\"],\"adultAllowed\":true,\"acceptedEula\":13,\"rating\":0,\"authoritiesInfo\":{\"issuedAuthorities\":[\"U_ASSIGNMENTS_VIEW\",\"U_ASSIGNMENTS_HISTORY\",\"APP_USER\",\"U_MESSAGES_CREATE\",\"U_MESSAGES_VIEW\",\"U_FORUM_VIEW\",\"U_PROFILE_VIEW\",\"U_WALLETS_EDIT\",\"U_TRANSACTIONS_VIEW\",\"U_FORUM_EDIT\",\"U_ASSIGNMENTS_UNDERTAKE\",\"U_TRANSACTIONS_CREATE\",\"U_ASSIGNMENTS_SUBMIT\",\"U_PROFILE_EDIT\",\"APP\"],\"notIssuedAuthoritiesReasons\":{}},\"balance\":\"0.191\",\"blockedBalance\":\"0.035\",\"regionId\":20741}'],\n",
    "#       dtype=object)\n",
    "# {\n",
    "#   \"acceptanceDetails\": {\n",
    "#     \"postAccept\": false\n",
    "#   },\n",
    "#   \"activeAssignments\": [\n",
    "#     {\n",
    "#       \"expireTime\": \"2022-01-20T19:37:28.194\",\n",
    "#       \"id\": \"00001086d6--61e9b930a2d62b2b56644596\",\n",
    "#       \"reward\": 0.3,\n",
    "#       \"secondsLeft\": 193\n",
    "#     }\n",
    "#   ],\n",
    "#   \"availability\": {\n",
    "#     \"available\": true\n",
    "#   },\n",
    "#   \"groupUuid\": \"9455a911-5624-4951-9f4e-ea09cc1cc5f5\",\n",
    "#   \"lightweightTec\": {\n",
    "#     \"assignmentConfig\": {\n",
    "#       \"issuing\": {\n",
    "#         \"type\": \"AUTOMATIC\"\n",
    "#       },\n",
    "#       \"maxDurationSeconds\": 200,\n",
    "#       \"reward\": 0.3\n",
    "#     },\n",
    "#     \"description\": \"Answer the questions in the survey. Choose one or more options or write your own answer\",\n",
    "#     \"hasInstructions\": true,\n",
    "#     \"iframeSubdomain\": \"54f8685950e9694b99faccce011a21df\",\n",
    "#     \"mayContainAdultContent\": false,\n",
    "#     \"poolId\": 1083094,\n",
    "#     \"poolStartedAt\": \"2022-01-20T19:01:57.121\",\n",
    "#     \"projectId\": 89244,\n",
    "#     \"projectMetaInfo\": {\n",
    "#       \"experimentMeta\": {},\n",
    "#       \"projectId\": 89244\n",
    "#     },\n",
    "#     \"requesterInfo\": {\n",
    "#       \"id\": \"54f8685950e9694b99faccce011a21df\",\n",
    "#       \"name\": {\n",
    "#         \"EN\": \"davidjohnsonits\"\n",
    "#       },\n",
    "#       \"trusted\": false\n",
    "#     },\n",
    "#     \"snapshotMajorVersion\": 1,\n",
    "#     \"snapshotMajorVersionActual\": true,\n",
    "#     \"snapshotMinorVersion\": 2,\n",
    "#     \"title\": \"Survey One David\",\n",
    "#     \"trainingConfig\": {\n",
    "#       \"training\": false\n",
    "#     }\n",
    "#   },\n",
    "#   \"refUuid\": \"a95bab84-7c39-45bc-9268-2f474011c0ae\",\n",
    "#   \"taskDetails\": {\n",
    "#     \"averageSubmitTimeSec\": 13,\n",
    "#     \"moneyAvgHourly\": 83.07692316\n",
    "#   },\n",
    "#   \"trainingDetails\": {\n",
    "#     \"training\": false\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# df_toloka.platform.unique()\n",
    "# array([ 0., nan])\n",
    "\n",
    "# df_toloka.subtype.unique()\n",
    "# array(['TASK_STARTED', 'TASKS_LIST', 'TASK_SUBMITED', 'OTHER',\n",
    "#        'FINISHED_TASK', 'SYSTEM', 'GENERAL', 'ADDED_TASK', 'META_DATA',\n",
    "#        'UNKNOWN', 'TASK_QUEUE', 'WORKER_QUALIFICATIONS',\n",
    "#        'WORKER_DASHBOARD', 'WORKER_EARNINGS', 'WORKER_EARNINGS_DETAILS',\n",
    "#        'TASK_INFO', 'MESSAGES_READ', 'TASK_TIMEOUT', 'REFERRAL',\n",
    "#        'NOTIFICATIONS', 'MESSAGES_REQUESTER', 'MESSAGES_OUTBOX',\n",
    "#        'MESSAGES_ADMIN', 'MESSAGES_NOTIFICATION'], dtype=object)\n",
    "\n",
    "# df_toloka.type.unique()\n",
    "# array(['WORKING', 'SEARCHING', 'OTHER', 'LOGS', 'CONFIG', 'API', 'SYSTEM',\n",
    "#        'UNKNOWN', 'PROFILE', 'COMMUNICATION', 'REJECTED'], dtype=object)\n",
    "\n",
    "# df_toloka.ordinal.unique()\n",
    "# array([1, 2])\n",
    "\n",
    "# df_toloka.unnamed.unique()\n",
    "# array([        nan, 0.00000e+00, 2.89000e+02, ..., 3.79992e+05,\n",
    "#        3.25610e+04, 3.61200e+04])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the unit of analysis?\n",
    "\n",
    "## How many observations in total are in the data set? \n",
    "\n",
    "## How many unique observations are in the data set? \n",
    "\n",
    "## What time period is covered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief summary of any data cleaning steps you have performed. For example, are there any particular observations / time periods / groups / etc. you have excluded?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of outcome with an appropriate visualization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of key predictors with appropriate visualization techniques that compare predictors to the response. You should investigate all predictors in your data as part of your project. For the purpose of this assignment, pick the one or two predictors that you think are going to be most important in explaining the outcome. Your selection of predictors can either be guided by your domain knowledge or be the result of your EDA on all predictors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc6300",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
