{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='heading'>\n",
    "    <div style='float:left;'><h1>CPSC 4300/6300: Applied Data Science</h1></div>\n",
    "    <img style=\"float: right; padding-right: 10px; width: 65px\" src=\"https://bsethwalker.github.io/assets/img/clemson_paw.png\">\n",
    "</div>\n",
    "\n",
    "## Course Project (Online Workers) - Checkpoint 1\n",
    "\n",
    "**Clemson University**<br>\n",
    "**Fall 2023**<br>\n",
    "**Instructor(s):** Nina Hubig <br>\n",
    "**Project Team:**\n",
    "<ul>\n",
    "    <li>David Croft <dcroft@g.clemson.edu></li>\n",
    "    <li>Stephen Becker <sgbecke@g.clemson.edu></li>\n",
    "    <li>Tony Hang <qhang@g.clemson.edu></li>\n",
    "    <li>Zachary Trabookis <ztraboo@clemson.edu></li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "\n",
       "div.heading {\n",
       "margin-bottom: 25px;\n",
       "height: 75px;\n",
       "}\n",
       "\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "    \n",
       "    background: rgba(245, 102, 0, .75);\n",
       "    border-color: #E9967A;\n",
       "    border-left: 5px solid #522D80; \n",
       "    padding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "    background-color: #fce8e8;\n",
       "    border-color: #E9967A; \t\n",
       "    border-left: 5px solid #800080; \n",
       "    padding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "    font-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "    background-color: #DDDDDD;\n",
       "    border-color: #E9967A; \t\n",
       "    border-left: 5px solid #800080; \n",
       "    padding: 0.5em;\n",
       "    font-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "    background-color: #AEDE94;\n",
       "    border-color: #E9967A; \t \n",
       "    border-left: 5px solid #800080; \n",
       "    padding: 0.5em;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://bsethwalker.github.io/assets/css/cpsc6300.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Goals\n",
    "\n",
    "* Summary of the data set that, at a minimum, answers the following questions: What is the unit of analysis? How many observations in total are in the data set? How many unique observations are in the data set? What time period is covered?\n",
    "  \n",
    "* Brief summary of any data cleaning steps you have performed. For example, are there any particular observations / time periods / groups / etc. you have excluded?\n",
    "  \n",
    "* Description of outcome with an appropriate visualization technique.\n",
    "  \n",
    "* Description of key predictors with appropriate visualization techniques that compare predictors to the response. You should investigate all predictors in your data as part of your project. For the purpose of this assignment, pick the one or two predictors that you think are going to be most important in explaining the outcome. Your selection of predictors can either be guided by your domain knowledge or be the result of your EDA on all predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Set the max columns to none. This allows all the columns to display for the dataframes.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "# Generic functions for cleaning the data\n",
    "def convert_epoch_time_to_datetime(epoch_time):\n",
    "    \"\"\"\n",
    "    Takes an epoch timestamp and converts it to datetime format\n",
    "    Ref: \n",
    "    https://www.pythonforbeginners.com/basics/convert-epoch-to-datetime-in-python\n",
    "    https://stackoverflow.com/questions/49710963/converting-13-digit-unixtime-in-ms-to-timestamp-in-python \n",
    "    \"\"\"\n",
    "\n",
    "    converted_date = None\n",
    "    try:\n",
    "        # Divide by 1,000 to remove ms time\n",
    "        converted_date = datetime.fromtimestamp(int(epoch_time)/1000).isoformat()\n",
    "    except ValueError as err:\n",
    "        # print(f\"Cannot convert epoch time {epoch_time} to isodate {err}\")\n",
    "\n",
    "        try:\n",
    "            date.fromisoformat(str(epoch_time))\n",
    "        except ValueError as err:\n",
    "            # Value passed is already in the correct `iso` format. Nothing else to do here.\n",
    "            return epoch_time\n",
    "    \n",
    "    return converted_date\n",
    "\n",
    "# Testing the epoch time conversion\n",
    "# convert_epoch_time_to_datetime(1588990000000) # '2020-05-08T22:06:40'\n",
    "# convert_epoch_time_to_datetime(1588994215395)   # '2020-05-08T23:16:55.395000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "import flatdict\n",
    "\n",
    "from collections.abc import MutableMapping\n",
    "\n",
    "def flatten_json_column(json_cols):\n",
    "    \"\"\"\n",
    "    This function flattens JSON columns to individual columns\n",
    "    It merges the flattened dataframe with expected dataframe to capture missing columns from JSON\n",
    "    :param df: Crowd Work Data CSV raw dataframe\n",
    "    :param json_cols: custom data columns in CSV's\n",
    "    :param custom_df: expected dataframe\n",
    "    :return: returns df pandas dataframe\n",
    "\n",
    "    Ref: \n",
    "    https://github.com/vvgsrk/ParseCSVContainsJSONUsingPandas/tree/main\n",
    "    https://avithekkc.medium.com/how-to-convert-nested-json-into-a-pandas-dataframe-9e8779914a24\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure to sort the `na_positions` last because this could effect how many columns\n",
    "    # that the nested column values are shown. If the nested column value is `NaN` first then\n",
    "    # nothing will get populated for those nested column fields. (e.g. `c4_project.hit_requirements`)\n",
    "    # Note: Comment out the fields that you don't want to show up in the final dataframe.\n",
    "    struct_data_json = {\n",
    "        \"task_id\": [None],\n",
    "        \"assignment_id\": [None],\n",
    "        \"accepted_at\": [None],\n",
    "        \"deadline\": [None],\n",
    "        \"time_to_deadline_in_seconds\": [None],\n",
    "        \"state\": [None],\n",
    "        \"question.value\": [None],\n",
    "        \"question.type\": [None],\n",
    "        # \"question.attributes\": [None],\n",
    "        \"question.attributes.FrameSourceAttribute\": [None],\n",
    "        \"question.attributes.FrameHeight\": [None],\n",
    "        \"project.hit_set_id\": [None],\n",
    "        \"project.title\": [None],\n",
    "        \"project.requester_id\": [None],\n",
    "        \"project.requester_name\": [None],\n",
    "        \"project.description\": [None],\n",
    "        \"project.assignment_duration_in_seconds\": [None],\n",
    "        \"project.creation_time\": [None],\n",
    "        \"project.assignable_hits_count\": [None],\n",
    "        \"project.latest_expiration_time\": [None],\n",
    "        \"project.caller_meets_requirements\": [None],\n",
    "        \"project.caller_meets_preview_requirements\": [None],\n",
    "        \"project.last_updated_time\": [None],\n",
    "        \"project.monetary_reward.currency_code\": [None],\n",
    "        \"project.monetary_reward.amount_in_dollars\": [None],\n",
    "        # \"project.hit_requirements.qualification_type_id\": [None],\n",
    "        # \"project.hit_requirements.comparator\": [None],\n",
    "        # \"project.hit_requirements.worker_action\": [None],\n",
    "        # \"project.hit_requirements.qualification_values\": [None],\n",
    "        # \"project.hit_requirements.caller_meets_requirement\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.qualification_type_id\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.name\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.visibility\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.description\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.has_test\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.is_requestable\": [None],\n",
    "        # \"project.hit_requirements.qualification_type.keywords\": [None],\n",
    "        # \"project.hit_requirements.caller_qualification_value.integer_value\": [None],\n",
    "        # \"project.hit_requirements.caller_qualification_value.locale_value.country\": [None],\n",
    "        # \"project.hit_requirements.caller_qualification_value.locale_subdivision\": [None],\n",
    "        \"project.requester_url\": [None],\n",
    "        \"expired_task_action_url\": [None],\n",
    "        \"task_url\": [None]\n",
    "    }\n",
    "\n",
    "    def _flatten_dict(d: MutableMapping, sep: str= '.') -> MutableMapping:\n",
    "        \"\"\"\n",
    "        Take in \n",
    "        \"\"\"\n",
    "        [flat_dict] = pd.json_normalize(data=d, sep=sep, max_level=None).to_dict(orient='records')\n",
    "        return flat_dict\n",
    "\n",
    "    try:\n",
    "        df_temp = pd.DataFrame(struct_data_json)\n",
    "\n",
    "        # If c4 `nan` value is passed, do nothing except return empty dataframe.\n",
    "        # If c4 has a string dicionary, then build new dataframe from it.\n",
    "        if isinstance(json_cols, str):\n",
    "            # Convert the input (str) to (dict) type.\n",
    "            # Build a flattened dictionary before sending to Pandas to `json_normalize`\n",
    "            dict_json_flattened = _flatten_dict(json.loads(json_cols))\n",
    "\n",
    "            # Explicitly remove this column because it's a nested list and is hard to flatten.\n",
    "            # Plus this column doesn't have any values that we need for our model.\n",
    "            del dict_json_flattened[\"project.hit_requirements\"]\n",
    "\n",
    "            df_temp = pd.DataFrame([dict_json_flattened])\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"flatten_json_columns: Invalid JSON argument passed - json.JSONDecodeError: {e}\")\n",
    "\n",
    "    # Return dataframe with flatten columns and 'c4.' prefix.\n",
    "    return df_temp.add_prefix('c4.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning: Reading in the telemetry data (Amazon Mechanical Turk (AMT))\n",
    "We read in and clean the data from `amazon_mechanical_turk_records.csv`.\n",
    "\n",
    "#### Background Research Paper\n",
    "Research Paper: *Quantifying the Invisible Labor in Crowd Work*\n",
    "\n",
    "https://dl.acm.org/doi/abs/10.1145/3476060?casa_token%3Dw4mZH0IjVgsAAAAA:XBgWg_Oq0TtNVqH8SzCxl2fXU_fZ9bzQ6g22QkI0odMy5NKW2EJdYrOaqxu_2NIqJs-rA_sM1sbT\n",
    "\n",
    "A browser plugin was used to collect the data. \n",
    "- https://github.com/GigPlatform/toloka-web-extension (has not been used for any analysis on any paper)\n",
    "- https://github.com/anonym-research/invisible-labor (was used for an invisible labor analysis but any predictive analysis has been performed)\n",
    "\n",
    "**List of available variables:** \n",
    "\n",
    "- **c1 (ID)**: continuous\n",
    "- **c2 (current)**: url, site visited by the worker (while working)\n",
    "- **c3 (event)**: categorical, 18 values (['PAGE_LOAD', 'PAGE_BLUR', 'TAB_CHANGE', 'PAGE_FOCUS', 'PAGE_CLICK', 'PAGE_SCROLL', 'PAGE_LAST', 'PAGE_CLOSE', 'INTERNALURL', 'PAGE_KEY', 'PAGE_INACTIVITY', 'TAB_CLOSED', 'EXTERNALURL', 'PAGE_REACTIVATE', 'SYSTEM_DISABLED_WORKING', 'SYSTEM_ENABLED_WORKING', 'SYSTEM_ENABLED', 'SYSTEM_DISABLED'])\n",
    "  - The web browser plugin recorded multiple events, the most relevant is PAGE_LOAD, other events can provide repetitive information.\n",
    "- **c4 (extra)**: json object {task_id, assignment_id, ...} – may include NaN values\n",
    "  - It provides a JSON object with the specificities of the tasks, it only has values for certain events.\n",
    "  - Are there any values out of this list that we should pay particular attention to?\n",
    "    - Yes, I recommend parsing the JSON so you can find information about the task, including how much was paid.\n",
    "- **c5 (platform)**: categorial, 5 values ['OTHER', 'MTURK', 'FIVERR', 'UPWORK', 'FREELANCER']\n",
    "  - The work platform in which the worker was working on (it is usually constant)\n",
    "- **c6 (skip)**: categorial, 2 values (0: no complete, 1: complete) – may include NaN values\n",
    "  - That was a field that was not used\n",
    "  - You mentioned skip. Does this represent if the task was completed or skipped?\n",
    "    - I do not remember the purpose of that field, maybe was not used\n",
    "- **c7 (subtype)**: categorial, 29 values ['OTHER', 'TASK_STARTED', 'ADDED_TASK', 'TASK_SUBMITED', 'FINISHED_TASK', 'TASKS_LIST', 'WORKER_DASHBOARD', 'UNKNOWN', 'TASK_FRAME', 'TASK_PREVIEW', 'TASK_INFO', 'TASK_RETURNED', 'PLATFORM_LOGIN', 'TASK_QUEUE', 'TASK_SKIP', 'WORKER_EARNINGS_DETAILS', 'TASK_TIMEOUT', 'WORKER_EARNINGS', 'WORKER_QUALIFICATIONS', 'TASKS_PER_REQUESTER', 'MESSAGES_SEND', 'TASKS_LIST_FILTER', 'WORKER_QUALIFICATIONS_PENDING', 'TASKS_PREVIEW', 'PLATFORM_HELP', 'TASKS_PROJECTS', 'TASKS_DETAILS', 'MESSAGES_READ', 'TASKS_APPLY']\n",
    "  - It defines if a worker is listing the tasks available, if a task just started, or if a task was completed (submitted)\n",
    "  - Is there a description that we can lookup to find what these event values mean?\n",
    "    - FINISHED_TASK == TASK_SUBMITED both refers to task completed (submitted is when it was recently submitted and finished when the next URL was loaded)\n",
    "    - TASK_RETURNED When the worker decided not to work on a task\n",
    "- **c8 (time)**: continuous, datetime (milliseconds), 1970 start date Unix Time (Week, Month, Day, Hours, Minutes, Seconds)\n",
    "  - This is a timestamp in milliseconds. You have to convert to a date, it contains day, month, year, hour, minute, second.\n",
    "  - You can use any function that converts from timestamp to datetime.\n",
    "  - What does this time represent? Task completed?\n",
    "    - Time of the event, remember that this is an event log, every event happened at this time. Time series analysis is a common approach to use.\n",
    "  - Does this time represent when the worker did the event recorded in c9 (type)?\n",
    "    - Worker was working \n",
    "    - Communicating by sending messages in the platform \n",
    "    - Searching for tasks\n",
    "    - Visiting their profile\n",
    "    - That is correct. it is the time at which it was recorded\n",
    "- **c9 (type)**: categorial, 10 values ['OTHER', 'WORKING', 'LOGS', 'SEARCHING', 'PROFILE', 'UNKNOWN', 'REJECTED', 'COMMUNICATION', 'LEARNING', 'PROPOSAL']\n",
    "  - It identify if at that time the worker was working, communicating by sending messages in the platform, searching for tasks, visiting their profile. There are other types that mean that the workers changed the CONFIG of the web plugin or the API of Toloka retrieved new tasks.\n",
    "  - Do you have a description for all these events?\n",
    "    - I do not have a data dictionary but these are separated by the events from the worker in the interface: worker was working, communicating by sending messages in the platform, searching for tasks, visiting their profile. The other events do not represent an activity of the worker but a state of the plugin, does not mean any worker activity these only got extra information from the web plugin.\n",
    "- **c10 (user)**: categorical, 120 values unique – Todo: Need to verify this is correct field value.\n",
    "  - User ID\n",
    "- **c11**: Not relevant (an activity was taken after being recommended)\n",
    "- **c12**: Not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def flatten_user_to_csv(user_id: int, df_user: pd.DataFrame, csv_output_path: str=\"\"):\n",
    "    \"\"\"\n",
    "    Read in the cleaned Amazon MT dataset and write it out for a particular user (e.g `ae862298385abab2a0a1619f8cedef9d`)\n",
    "    Convert the `c4` event column by flattening most dict values into separate columns and \n",
    "    write out to temporary *.csv to run limited records moving forward.\n",
    "\n",
    "    Parameters\n",
    "    user_id (int): c10 (user) field in the original dataset\n",
    "    df_user (pd.DataFrame): Pandas dataframe information for the user on the original dataset.\n",
    "    debug (bool): indicate\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the data into a dataframe and flatten 'c4 (event)' column to new columns.\n",
    "\n",
    "    # # Suggest writing this transformed data out to a file to read in that transformed file for further processing.\n",
    "    # df_temp_user = pd.read_csv(FLATTENED_USER_CSV_PATH, encoding='utf-8', header=\"infer\")\n",
    "\n",
    "    # df_temp_user.sort_values(by=['user', 'time'], ascending=[True, True]).to_csv(FLATTENED_USER_CSV_PATH,\n",
    "    #                                                                              encoding='utf-8', header=True, columns=columns, index=False, mode=\"w\")\n",
    "    \n",
    "    # Convert the milliseconds c8 (time) to an iso datetime format.\n",
    "    # For some reason writing out to temporary csv here helps when joining with the c4 (extra) flattened information.\n",
    "    # If we forget this step the concat of c4 (extra) to the original dataframe gets messed up.\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    TIMECONVERTED_USER_CSV_PATH = f\"../data/amazon_mechanical_turk_records_{user_id}.csv\"\n",
    "\n",
    "    # Convert the epoch timestamp to datetime and write out to temporary csv file.\n",
    "    df_user['time'] = df_user.time.map(convert_epoch_time_to_datetime)\n",
    "    df_user.to_csv(TIMECONVERTED_USER_CSV_PATH, encoding='utf-8', header=True, index=False, mode=\"w\")\n",
    "\n",
    "    # Droping the Dataframe to avoid duplicates when reading in again from csv after transforming the epic timestamp.\n",
    "    df_user.drop(df_user.index, inplace=True)\n",
    "\n",
    "    # Read in the transformed epoch time values and perform further processing with c4 (extra) column.\n",
    "    df_user = pd.read_csv(TIMECONVERTED_USER_CSV_PATH, encoding='utf-8', header=\"infer\")\n",
    "    os.remove(TIMECONVERTED_USER_CSV_PATH)\n",
    "    # print(\"Removed temporary time converted file {TIMECONVERTED_USER_CSV_PATH}.\")\n",
    "    # ------------------------------------------------------------------------------------\n",
    "\n",
    "    df_c4_flattened = pd.DataFrame()\n",
    "    for i, j in df_user.iterrows():\n",
    "        if j[\"c4\"]:\n",
    "            # https://stackoverflow.com/questions/33094056/is-it-possible-to-append-series-to-rows-of-dataframe-without-making-a-list-first\n",
    "            series_temp = flatten_json_column(j[\"c4\"]).iloc[0].to_frame().T\n",
    "            df_c4_flattened = pd.concat([df_c4_flattened, series_temp], ignore_index=True) \n",
    "\n",
    "    df_user = pd.concat([df_user, df_c4_flattened], axis=\"columns\")\n",
    "    del df_user[\"c4\"]   \n",
    "\n",
    "    # Data is sorted by c8 (time) field to ensure the events are in order.\n",
    "    # This sorting by c8 (time) is important before we perform calculations on invisible labor time.\n",
    "    df_user.sort_values(by=['time'], ascending=[True], inplace=True)\n",
    "\n",
    "    # Write out the flattened user csv information.\n",
    "    df_user.to_csv(csv_output_path, encoding='utf-8', header=True, index=False, mode=\"w\")\n",
    "\n",
    "    print(f\"Created flattened csv {csv_output_path} for user {user_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def transform_users_to_flattened_csv(csv_output_path: str=\"\", limit_users: int=None, debug: bool=False):\n",
    "    \"\"\"\n",
    "    Read in the original Amazon MT dataset and write it out for a particular user (e.g. `ae862298385abab2a0a1619f8cedef9d`)\n",
    "    Convert the `c8` time column to epoch time and write out to temporary *.csv to run limited records moving forward.\n",
    "    \"\"\"\n",
    "\n",
    "    DATASET_AWS_MTURK_CSV_PATH = \"../data/amazon_mechanical_turk_records.csv\"\n",
    "\n",
    "    # Define columns for data\n",
    "    columns = ['c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'time', 'c9', 'user']\n",
    "\n",
    "    # Read the data into a dataframe for further manipulation.\n",
    "\n",
    "    # https://stackoverflow.com/questions/13651117/how-can-i-filter-lines-on-load-in-pandas-read-csv-function\n",
    "    # iter_csv_default = pd.read_csv(DATASET_AWS_MTURK_CSV_PATH, encoding='utf-8', header=None, names=columns, low_memory=False, iterator=True, chunksize=1000)\n",
    "    # df_default = pd.concat([chunk[chunk['user'] == \"ae862298385abab2a0a1619f8cedef9d\"] for chunk in iter_csv_default])\n",
    "\n",
    "    # 4m 18.8s – Suggest writing this transformed data out to a file to read in that transformed file for further processing.\n",
    "    df_default = pd.read_csv(DATASET_AWS_MTURK_CSV_PATH, encoding='utf-8', header=None, names=columns, low_memory=False)\n",
    "    \n",
    "    # # Grab number of unique users based on dataset provided.\n",
    "    # if limit_users is not None:\n",
    "    #     unique_users = df_default['user'].dropna().unique().tolist()[:limit_users]\n",
    "    # else:\n",
    "    #     # Grab all users if we not limiting number of users.\n",
    "    #     # Warning: Depending on how large the dataset is this could take a while to process all users.\n",
    "    #     # Recommend limiting the numbers of users before running all.\n",
    "    #     unique_users = df_default['user'].dropna().unique().tolist()\n",
    "    unique_users = [\n",
    "        \"4bc91376a0d164c5594c524788723d\",\n",
    "        \"8697ae35eb3bf8ce8d5669260897895\",\n",
    "        \"ffb0c8b77e4f393844efa24f5254e9b\",\n",
    "        \"ce19344b46e1aa94598dfe0e36e40\",\n",
    "        \"ba633f7f73ff50c74c8539f09a8e55e\",\n",
    "        \"612d686264ac53a67ba88dec91238837\",\n",
    "        \"d94eed14283e3cf95445a2d2d0c732ea\",\n",
    "        \"c45ae0d5903111f7536eff13443b121f\",\n",
    "        \"b3768cd510cd9563f4bbb8426b943c81\",\n",
    "        \"3a305b70db8874759c2dfc4228ae05f\",\n",
    "        \"c11a1f6f688f48c521d071ae29570e6\",\n",
    "        \"f2687f9a5c9a26cc8936b2d93be8ec8\",\n",
    "        \"c35975cf36e8a25076da28a82ce70be\",\n",
    "        \"2a1d22d7101b7b66e06530c86d1b57b0\",\n",
    "        \"ae1d53c23b4e45aa85e3b668bef64ab\",\n",
    "        \"e4629593798243d93e1d3b8af9912110\",\n",
    "        \"8bf2181261573e7e329c40614c279719\",\n",
    "        \"19a4b3e35b14fdfd95020d7a8483a4\",\n",
    "        \"5b93a634b0dfadc05a938f9f37677b4\",\n",
    "        \"1164fceb602e16e44dc3f6e49ec51\",\n",
    "        \"de6f341f486569af7f658f13a698ea4e\",\n",
    "        \"82f7a1a4be3dff5b63c145c97ba748a1\",\n",
    "        \"99bce524fe0a27863aba87f905e34d6\",\n",
    "        \"fc66131fcea7f4f2f80278189ed1f8\",\n",
    "        \"e61f4320ad212464d4ee2c764a993b\",\n",
    "        \"1fbbe51c2b7814f7810188ddc19f4ed\",\n",
    "        \"f4b1f58182ccaef03a28fed5539a89f9\",\n",
    "        \"c99687531ab1967a51302de2d4572\",\n",
    "        \"b8e3eb35cabedb645e1a279ad96d567\",\n",
    "        \"01ee4a227223252d9dd5862d122f9e\",\n",
    "        \"4912f382b67922416cfea2359a828d1\",\n",
    "        \"a87398e4bd28fe7b8077886098c6e\",\n",
    "        \"32d3b5ecde232650c24cd36bd3ad2d1d\",\n",
    "        \"f3d82fd8fa6a5eb7555c0abf43edd23\",\n",
    "        \"a0f699a13a335bda7a73115e5f4ee08f\",\n",
    "        \"442f53cfa6a817e452a1f769f3c5d2\",\n",
    "        \"aaf05f30ee38781cae9bbca0b5acdac0\",\n",
    "        \"8eacce56d38f1f69950fb77cd1537\",\n",
    "        \"9eabb6c86fa8b6152aa3a6588581edd5\",\n",
    "        \"c2f95332a399afb229355bd932754646\",\n",
    "        \"ce3faf44e77f5dda69a3a6633621ab\",\n",
    "        \"88a8c56fc6b65633e7a1975bc29f1fec\",\n",
    "        \"dcff9ab0bc8c3fa1a9ee573585a5889\",\n",
    "        \"f0bd27d3aaa7b49f4edfc5f72f6d99\",\n",
    "        \"fef9998b1fa31a786993019be5461fa\",\n",
    "        \"cabad7e3a75eb660956c8bf6cbf3c07\",\n",
    "        \"2e259988387deea78754c4cde4cc8cc\",\n",
    "        \"ed8a6079f5230fa4f787c72b1993bba\",\n",
    "        \"dc79c0907046d4df5ca45b213681dc98\",\n",
    "        \"fd89522795bd36f4ee4ae5f0a11c71\",\n",
    "        \"7f2f41e71876dd8a981dd7c7bb5fa3f1\",\n",
    "        \"fccf29e7a47d097ee4695db8aa1bf5\",\n",
    "        \"baa323f53929015db7a2c69d97bb4d9\",\n",
    "        \"7c8f72c6eb861ce6fd35a6debce1161b\",\n",
    "        \"2fa31ea51f645df2cb9aa14b58c4183b\",\n",
    "        \"c2d79d4e92f4215c93f17ea1acf38798\",\n",
    "        \"caf59447d7eb2954a3657b638b590c4\",\n",
    "        \"74cd6c2ee9d9709ce0a534388793de\",\n",
    "        \"bc42df9a3373d27be9780fd6ae128\",\n",
    "        \"ceef928ee76bf9a879d35998e60a4be\",\n",
    "        \"2ecafc36ff07621e0ce652b264b8bdf\",\n",
    "        \"623963faa7268752f2bad8b37b7585\",\n",
    "        \"d73b4fa896b26f5a91a1f41d7b5fdd1\",\n",
    "        \"f04ca22c35c85fe6562a3b4d159fb79\",\n",
    "        \"93c4b02efd6dd4e0f7a9a3d3a6466670\",\n",
    "        \"14c6ecb74490e12bb7f8108cb4a9c34e\",\n",
    "        \"8ade05afb024cb45efadebcdec7a32\",\n",
    "        \"dc6e62b296f81ec347d5de0911fa719\",\n",
    "        \"d7e17849a338aad1a22d3ec665846d4f\",\n",
    "        \"4bc966abc2d6aead5bfbb5e47fbab\",\n",
    "        \"2be6ab2acaf924bc5adbc3f979b87\",\n",
    "        \"c137b2a837ec10c59778c65338d882\",\n",
    "        \"28cda45766dd7647cbd9ff4890fadd\",\n",
    "        \"dc5b9a35c8d99453ebcf929a9b38cc\",\n",
    "        \"9a13864813d25c911b8d107462f1aa\",\n",
    "        \"10a76d9ce5e5dfee6352614a9b18b6bc\",\n",
    "        \"241b12459cdd203778fd3d94e563a712\",\n",
    "        \"5356644850542c7e38a29248ab1f1b61\",\n",
    "        \"ab6c2eab2d5214e4a5205e2b7aa8db\",\n",
    "        \"25a05b19f637703073d22de66e69fc21\",\n",
    "        \"858afc5d972fc3a567d3d2a1d0e44698\",\n",
    "        \"66c189ce8436ff6335731bc1159f7f\",\n",
    "        \"42bcd2adfa6d5d6a493259e1cb2d9f\",\n",
    "        \"eea8927de66f1de3fa354ba4eb95a20\",\n",
    "        \"83fbc02785ea9b9d6951be0e201ee1\",\n",
    "        \"6aba944ddaef676f885c762196691d4\",\n",
    "        \"83e55147fca340899e2119ad2c2820c6\",\n",
    "        \"f0ac4982ad88a5b45e5923a71242145\",\n",
    "        \"cb56c5429418696a451c3b4cfcec8d6\",\n",
    "        \"86e0382753203b338b5117a461471080\",\n",
    "        \"b5437f47f866feced73d66ff3db3e4\",\n",
    "        \"935a391af179718263b18f56a2bf36f2\",\n",
    "        \"3019198226cdb429651d3758afa8b59b\",\n",
    "        \"47a95b7352b2928f249f434d10ee32\",\n",
    "        \"7dd3d86e412c9c22b2d48acb2a3924a0\",\n",
    "        \"80ab8240161d8f53a67d927ef37c225e\",\n",
    "        \"586b9c67fff494279a99be871455838\",\n",
    "        \"3eb9e4726656aef256b9d916f0c7\",\n",
    "        \"f0d0c143cb4df66a96b931fc14ae7c\",\n",
    "        \"75b1987cc42221d4a11e74244fdc9ee\",\n",
    "        \"84da44b85948b817e5945afde6f0fb10\",\n",
    "        \"34dc96ce13cdc44f897e25eb4398079\",\n",
    "        \"9cfee018ac43b24823ace44ea835f2\"\n",
    "    ]\n",
    "    # unique_users = [\n",
    "    #     \"ae862298385abab2a0a1619f8cedef9d\",\n",
    "    #     \"149c64b9f9b890bcf32bd2dcf595fd\",\n",
    "    #     \"49e7482b6cda157f388c73b3bcc2ebfc\",\n",
    "    #     \"6c418c234637a262f4b3cc8fbb2ab683\",\n",
    "    #     \"13327b278744b9997a995fbfcc83d9e\",\n",
    "    #     \"376b1526a10bc9d9ca8a71b51d951f6\",\n",
    "    #     \"f0dacff155d4665310aa9c1b5b76c6c\",\n",
    "    #     \"d7a1761ca2820df428de327256d16\",\n",
    "    #     \"2cd3cfa492db635891e3ea3b402982\",\n",
    "    #     \"bdb1e5a4546e1de80f3e8d65ed5a81\",\n",
    "    #     \"1c4a09c264cc784a227191d775b213b\",\n",
    "    #     \"112a14c770ec5b7f907fd95b5c13c\",\n",
    "    #     \"7c70f88eb8d1574e2bfd7b7b883c3a64\",\n",
    "    #     \"f7894afaa9c87d30f7b8a102e92479d\",\n",
    "    #     \"5ba2c2bc7b3bb19711ede5857611abc\",\n",
    "    # ]\n",
    "\n",
    "    # unique_users = [\"ae862298385abab2a0a1619f8cedef9d\", \"149c64b9f9b890bcf32bd2dcf595fd\", \"49e7482b6cda157f388c73b3bcc2ebfc\"]\n",
    "\n",
    "    # Write out all flatten files per user.\n",
    "    for user_id in unique_users:\n",
    "        FLATTENED_USER_CSV_PATH = f\"../data/amazon_mechanical_turk_records_{user_id}_flattened.csv\"\n",
    "        # df_temp_user = pd.DataFrame()\n",
    "        # df_temp_user = df_default[df_default.user == user_id].copy(deep=True)\n",
    "\n",
    "        # df_temp_user.sort_values(by=['user', 'time'], ascending=[True, True]).to_csv(FLATTENED_USER_CSV_PATH,\n",
    "        #             encoding='utf-8', header=True, columns=columns, index=False, mode=\"w\")\n",
    "        \n",
    "        # Write out user. Perform a copy of the dataset into 'df_user' to prevent altering the original 'df_default'.\n",
    "        flatten_user_to_csv(\n",
    "            user_id,\n",
    "            df_user=df_default[df_default.user == user_id],\n",
    "            csv_output_path=FLATTENED_USER_CSV_PATH\n",
    "            )\n",
    "    \n",
    "    # # Combine all flattened user files into one.\n",
    "    # if csv_output_path:           \n",
    "    #     df_flatten_users = pd.concat(\n",
    "    #         [\n",
    "    #             pd.read_csv(\n",
    "    #                 f\"../data/amazon_mechanical_turk_records_{user_id}_flattened.csv\", encoding='utf-8', header=\"infer\"\n",
    "    #                 ) for user_id in unique_users\n",
    "    #         ], ignore_index=True\n",
    "    #     )\n",
    "\n",
    "    #     # sort_values(by=['user', 'time'], ascending=[True, True])\n",
    "    #     df_flatten_users.to_csv(csv_output_path, encoding='utf-8', header=True, index=False, mode=\"w\")\n",
    "    #     print(f\"Created flattened csv for {unique_users} users at {csv_output_path}.\")\n",
    "\n",
    "    #     # Keep the csv around for each user to debug further.\n",
    "    #     if not debug:\n",
    "    #         for user_id in unique_users:\n",
    "    #             os.remove(FLATTENED_USER_CSV_PATH)\n",
    "    #             print(f\"Removed temporary flattened file {FLATTENED_USER_CSV_PATH}.\")\n",
    "    # else:\n",
    "    #     print(\"Cannot flatten users to csv because the output path is not specified.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLATTENED_USERS_CSV_PATH = f\"../data/amazon_mechanical_turk_records_users_flattened.csv\"\n",
    "LIMIT_USERS = None # 3\n",
    "\n",
    "# Call this function to create a separate *.csv for a particular user and use that moving forward.\n",
    "# Because this takes long to create the flatten files and combine them we only want to read the \n",
    "# original data we're just calling this once.\n",
    "# 5m 47.5s - first 3 users\n",
    "# transform_users_to_flattened_csv(csv_output_path=FLATTENED_USERS_CSV_PATH, limit_users=LIMIT_USERS, debug=True)\n",
    "\n",
    "# Read in flattened c4 csv for the user.\n",
    "df_amt = pd.read_csv(FLATTENED_USERS_CSV_PATH, encoding='utf-8', header=\"infer\", low_memory=False)\n",
    "\n",
    "# Sort by user then by time\n",
    "# df_amt = df_amt.sort_values(by=['user', 'time'], ascending=[True, True])\n",
    "\n",
    "# df_amt.head(30)\n",
    "# df_amt[df_amt[\"c4.task_id\"].notna()]\n",
    "# df_amt.iloc[0:16,:]\n",
    "# df_amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351686 entries, 0 to 351685\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                        Non-Null Count   Dtype  \n",
      "---  ------                                        --------------   -----  \n",
      " 0   c1                                            351686 non-null  int64  \n",
      " 1   c2                                            351686 non-null  object \n",
      " 2   c3                                            351686 non-null  object \n",
      " 3   c5                                            351686 non-null  object \n",
      " 4   c6                                            351686 non-null  float64\n",
      " 5   c7                                            351686 non-null  object \n",
      " 6   time                                          351686 non-null  object \n",
      " 7   c9                                            351686 non-null  object \n",
      " 8   user                                          351686 non-null  object \n",
      " 9   c4.task_id                                    13271 non-null   object \n",
      " 10  c4.assignment_id                              13271 non-null   object \n",
      " 11  c4.accepted_at                                13271 non-null   object \n",
      " 12  c4.deadline                                   13271 non-null   object \n",
      " 13  c4.time_to_deadline_in_seconds                13271 non-null   float64\n",
      " 14  c4.state                                      13271 non-null   object \n",
      " 15  c4.question.value                             13271 non-null   object \n",
      " 16  c4.question.type                              13271 non-null   object \n",
      " 17  c4.question.attributes.FrameSourceAttribute   13271 non-null   object \n",
      " 18  c4.question.attributes.FrameHeight            13271 non-null   float64\n",
      " 19  c4.project.hit_set_id                         13271 non-null   object \n",
      " 20  c4.project.title                              13271 non-null   object \n",
      " 21  c4.project.requester_id                       13271 non-null   object \n",
      " 22  c4.project.requester_name                     13271 non-null   object \n",
      " 23  c4.project.description                        13271 non-null   object \n",
      " 24  c4.project.assignment_duration_in_seconds     13271 non-null   float64\n",
      " 25  c4.project.creation_time                      13271 non-null   object \n",
      " 26  c4.project.assignable_hits_count              13271 non-null   float64\n",
      " 27  c4.project.latest_expiration_time             13271 non-null   object \n",
      " 28  c4.project.caller_meets_requirements          13271 non-null   object \n",
      " 29  c4.project.caller_meets_preview_requirements  13271 non-null   object \n",
      " 30  c4.project.last_updated_time                  13270 non-null   object \n",
      " 31  c4.project.monetary_reward.currency_code      13271 non-null   object \n",
      " 32  c4.project.monetary_reward.amount_in_dollars  13271 non-null   float64\n",
      " 33  c4.project.requester_url                      13271 non-null   object \n",
      " 34  c4.expired_task_action_url                    13271 non-null   object \n",
      " 35  c4.task_url                                   13271 non-null   object \n",
      "dtypes: float64(6), int64(1), object(29)\n",
      "memory usage: 96.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Look at the features\n",
    "df_amt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def locate_user_task_start(\n",
    "        df: pd.DataFrame,\n",
    "        user_id: str=\"\",\n",
    "        task_id: str=\"\",\n",
    "        task_started_pattern: str=\"https://worker.mturk.com/projects/(.*)/tasks/(.*)?assignment_id(.*)\"\n",
    "        ) -> (int, datetime):\n",
    "    \"\"\"\n",
    "    Search `c2` url and locate the first event where `c7 (subtype) == TASK_STARTED` and matches\n",
    "    the argument `task_id`.\n",
    "\n",
    "    This limits searching the whole dataset to find the first `c2` url where the task_id has \n",
    "    'TASK_STARTED' generated. Filtering data by c3 == ('PAGE_LOAD', 'TAB_CHANGE') and c7 == 'TASK_STARTED'\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk (need columns c2 (url) and c8 (time))\n",
    "    user_id (str): c10 (user) value.\n",
    "    task_id (str): c4.task_id value.\n",
    "    task_started_pattern (str): Regular expression pattern to identify 'TASK_STARTED'. Defaults to expression in the Chome plugin for AWS MTurk.\n",
    "\n",
    "    Returns:\n",
    "    id: c1 (id) location of dataset record.\n",
    "    datetime: Task id start date for first event 'TASK_STARTED'\n",
    "    \"\"\"\n",
    "\n",
    "    c1_id = None\n",
    "    c8_event_date = None\n",
    "    url_task_id = None\n",
    "\n",
    "    # Limit the amount of records needing searched.\n",
    "    df_task_started = df[\n",
    "        (df.user == user_id) & (\n",
    "            (\n",
    "                ((df.c7 == 'TASK_FRAME') | (df.c7 == 'TASK_STARTED')) & \n",
    "                (df.c9 == 'WORKING')\n",
    "            ) |\n",
    "            ((df.c7 == 'ADDED_TASK') & (df.c9 == 'LOGS'))\n",
    "        )\n",
    "        ].sort_values(by=['time'], ascending=[True])\n",
    "\n",
    "    for i, j in df_task_started.c2.items():\n",
    "        if re.search(\"https://worker.mturk.com/projects/(.*)/tasks/(.*)?assignment_id(.*)\", j, re.IGNORECASE):\n",
    "            url_task_id = j.split('/')[6].split('?')[0]\n",
    "        elif re.search(\"https://www.mturkcontent.com/dynamic/hit\\?assignmentId=(.*)\", j, re.IGNORECASE):\n",
    "            url_task_id = j.split('hitId=')[1].split('&')[0]\n",
    "\n",
    "        if url_task_id and (url_task_id == task_id):\n",
    "            try:\n",
    "                c1_id = df_task_started.c1[i]\n",
    "                c8_event_date = datetime.fromisoformat(df_task_started.time[i])\n",
    "                # print(f\"Found task {task_id} first 'TASK_STARTED': id {c1_id}, date {df_task_started.time[i]}\")    \n",
    "            except ValueError:\n",
    "                print(f\"Could not find 'TASK_STARTED' event date for task_id {task_id}\")\n",
    "            \n",
    "            # Make sure that we break here to ensure that we're just looking at the \n",
    "            # first 'TASK_STARTED' event time.\n",
    "            break\n",
    "\n",
    "    # We may have a situation where there is no 'TASK_STARTED' for the task but a 'ADDED_TASK' exists.\n",
    "    if not (c1_id and c8_event_date):\n",
    "        try:\n",
    "            # Limit the amount of records needing searched.\n",
    "            df_task_started = df[\n",
    "                (df.user == user_id) &\n",
    "                (df.c7 == 'ADDED_TASK') & \n",
    "                (df.c9 == 'LOGS') & \n",
    "                (df[\"c4.task_id\"] == task_id)\n",
    "                ].iloc[0]\n",
    "        \n",
    "            c1_id = df_task_started.c1\n",
    "            c8_event_date = datetime.fromisoformat(df_task_started.time)\n",
    "            # print(f\"Found task {task_id} first 'ADDED_TASK': id {c1_id}, date {df_task_started.time}\")    \n",
    "        except (IndexError, ValueError) as err:\n",
    "            print(f\"Could not find 'ADDED_TASK' event date for task_id {task_id}: {err}\")\n",
    "    else:\n",
    "        try:\n",
    "            # Check to see if we have an 'ADDED_TASK' before this 'c1_id' and use it instead if there.\n",
    "            # This is because the background process logs this event every 30 minutes and it does not\n",
    "            # abide by the event order.\n",
    "            # (e.g. 3E9ZFLPWO0KJDJRNXDHG072XWOZXIK)\n",
    "            df_added_task = df_task_started[(df_task_started.c7 == 'ADDED_TASK') & (df_task_started[\"c4.task_id\"] == task_id)].iloc[0]\n",
    "\n",
    "            # Only update if the df_added_task is before what we found previously.\n",
    "            # This would be the case if there was no c1_id previously set.\n",
    "            if df_added_task.c1 < c1_id:\n",
    "                c1_id = df_added_task.c1\n",
    "                c8_event_date = datetime.fromisoformat(df_added_task.time)\n",
    "        except IndexError as err:\n",
    "            # No previous 'ADDED_TASK' found, so we'll stick with what we found with 'TASK_STARTED'.\n",
    "            pass\n",
    "\n",
    "    return c1_id, c8_event_date\n",
    "\n",
    "\n",
    "def locate_user_task_end(\n",
    "        df: pd.DataFrame,\n",
    "        user_id: str=\"\",\n",
    "        task_id: str=\"\",\n",
    "        ) -> (int, datetime):\n",
    "    \"\"\"\n",
    "    Search for observations where last event is`c7 (subtype) == FINISHED_TASK` and matches\n",
    "    the argument `task_id`. \n",
    "\n",
    "    Note: We could have used `TASK_SUBMITTED`, however, we noticed some invisible tasks came after.\n",
    "    Carlos mentions the following:\n",
    "    \"FINISHED_TASK == TASK_SUBMITED both refers to task completed (submitted is when it was \n",
    "    recently submitted and finished when the next URL was loaded)\"\n",
    "\n",
    "    This limits searching the whole dataset to find the first `c7` subtype where the task_id has \n",
    "    'FINISHED_TASK' generated. Filtering data by c7 == 'FINISHED_TASK' and c9 (type) == 'LOGS'\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk (need columns c7 (subtype) and c9 (type))\n",
    "    user_id (str): c10 (user) value.\n",
    "    task_id (str): c4.task_id value.\n",
    "\n",
    "    Returns:\n",
    "    id: c1 (id) location of dataset record.\n",
    "    datetime: Task id end date for event 'FINISHED_TASK'\n",
    "    \"\"\"\n",
    "\n",
    "    c1_id = None\n",
    "    c8_event_date = None\n",
    "\n",
    "    try:\n",
    "        # Limit the amount of records needing searched to the first row.\n",
    "        df_task_finished = df[\n",
    "            (df.user == user_id) &\n",
    "            (df.c7 == 'FINISHED_TASK') & \n",
    "            (df.c9 == 'LOGS') &\n",
    "            (df[\"c4.task_id\"] == task_id)\n",
    "            ].iloc[0]\n",
    "\n",
    "        c1_id = df_task_finished.c1\n",
    "        c8_event_date = datetime.fromisoformat(df_task_finished.time)\n",
    "        # print(f\"Found task {task_id} first 'FINISHED_TASK': id {c1_id}, date {df_task_finished.time}\")    \n",
    "    except (IndexError, ValueError) as err:\n",
    "        print(f\"Could not find 'FINISHED_TASK' event date for task_id {task_id}: {err}\")\n",
    "    \n",
    "    return c1_id, c8_event_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'locate_user_task_start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint 1/online_workers_checkpoint1_aws_mturk.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=332'>333</a>\u001b[0m \u001b[39m# unique_tasks = ['3N5YJ55YXG3AVVNQ3MIU09Y29Q1AN7']\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=333'>334</a>\u001b[0m \u001b[39m# unique_tasks = ['3CMIQF80GN0XWBC81XEGMJKYJB8Q64']\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=334'>335</a>\u001b[0m \u001b[39m# [\"3W9XHF7WGLV68SQR2YVUGGPI6QVTK3\"] \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=335'>336</a>\u001b[0m \u001b[39m# [\"3KA7IJSNW54MTVXHF3TMHNX8OOTPBI\"]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=336'>337</a>\u001b[0m \u001b[39m# [\"3E9ZFLPWO0KJDJRNXDHG072XWOZXIK\"]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=338'>339</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(unique_tasks):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=340'>341</a>\u001b[0m     task_c1_id_start, task_start_date \u001b[39m=\u001b[39m locate_user_task_start(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=341'>342</a>\u001b[0m         df_amt_user, user_id, t, \u001b[39m\"\u001b[39m\u001b[39mhttps://worker.mturk.com/projects/(.*)/tasks/(.*)?assignment_id(.*)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=342'>343</a>\u001b[0m         )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=343'>344</a>\u001b[0m     task_c1_id_end, task_end_date \u001b[39m=\u001b[39m locate_user_task_end(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=344'>345</a>\u001b[0m         df_amt_user, user_id, t\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=345'>346</a>\u001b[0m         )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=347'>348</a>\u001b[0m     \u001b[39m# If we have record that doesn't have a start time then let's remove it.\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ztraboo/Dev/CPSC-6300/cpsc-6300-course-project/Checkpoint%201/online_workers_checkpoint1_aws_mturk.ipynb#X15sZmlsZQ%3D%3D?line=348'>349</a>\u001b[0m     \u001b[39m# Example: '302U8RURJY0UZS7SBXEJSBSUBN2VNK' doesn't include a 'ADDED_TASK' or 'TASK_STARTED' event.            \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'locate_user_task_start' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "def task_platform_name(df: pd.DataFrame, c1_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Returns platform name for the 'FINISHED_TASK' event.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk (need column c1 (id) for lookup)\n",
    "    c1_id: The c1 (id) value for lookup int the dataframe passed.\n",
    "\n",
    "    Returns:\n",
    "    str: Name of the platform where the worker task exists.\n",
    "    \"\"\"\n",
    "    requester_name = \"\"\n",
    "    try:\n",
    "        requester_name = df[df.c1 == c1_id].iloc[0][\"c5\"]\n",
    "    except (IndexError, TypeError) as err:\n",
    "        print(f\"Cannot locate platform name for the task at c1 == {c1_id} {err}\")\n",
    "\n",
    "    return requester_name\n",
    "\n",
    "\n",
    "def task_requester_name(df: pd.DataFrame, c1_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Returns project requester name for the 'FINISHED_TASK' event.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk (need column c1 (id) for lookup)\n",
    "    c1_id: The c1 (id) value for lookup int the dataframe passed.\n",
    "\n",
    "    Returns:\n",
    "    str: Requester of the current task.\n",
    "    \"\"\"\n",
    "    requester_name = \"\"\n",
    "    try:\n",
    "        requester_name = df[df.c1 == c1_id].iloc[0][\"c4.project.requester_name\"]\n",
    "    except (IndexError, TypeError) as err:\n",
    "        print(f\"Cannot locate project requester name at c1 == {c1_id} {err}\")\n",
    "\n",
    "    return requester_name\n",
    "\n",
    "\n",
    "def task_estimate_duration_in_seconds(df: pd.DataFrame, c1_id: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns 'c4.project.assignment_duration_in_seconds' duration in seconds from 'FINISHED_TASK' event.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk (need column c1 (id) for lookup)\n",
    "    c1_id: The c1 (id) value for lookup int the dataframe passed.\n",
    "\n",
    "    Returns:\n",
    "    int: Seconds of task total time estimate duration. This includes labor (working + invisible).\n",
    "    \"\"\"\n",
    "    duration = 0\n",
    "    try:\n",
    "        duration = int(df[df.c1 == c1_id].iloc[0][\"c4.project.assignment_duration_in_seconds\"])\n",
    "    except (IndexError, TypeError) as err:\n",
    "        print(f\"Cannot calculate task total time duration {err}\")\n",
    "\n",
    "    return duration\n",
    "\n",
    "\n",
    "def task_monetary_reward_in_dollars(df: pd.DataFrame, c1_id: int) -> float:\n",
    "    \"\"\"\n",
    "    Returns 'c4.project.monetary_reward.amount_in_dollars' duration in seconds from 'FINISHED_TASK' event.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk (need column c1 (id) for lookup)\n",
    "    c1_id: The c1 (id) value for lookup int the dataframe passed.\n",
    "\n",
    "    Returns:\n",
    "    int: US dollar amount for the task. This includes labor (working + invisible).\n",
    "    \"\"\"\n",
    "    monetary = 0\n",
    "    try:\n",
    "        monetary = float(df[df.c1 == c1_id].iloc[0][\"c4.project.monetary_reward.amount_in_dollars\"])\n",
    "    except (IndexError, TypeError) as err:\n",
    "        print(f\"Cannot calculate task monetary reward {err}\")\n",
    "\n",
    "    return monetary\n",
    "\n",
    "\n",
    "def task_assignable_hits_count(df: pd.DataFrame, c1_id: int) -> float:\n",
    "    \"\"\"\n",
    "    Returns 'c4.project.assignable_hits_count' value from 'FINISHED_TASK' event.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk (need column c1 (id) for lookup)\n",
    "    c1_id: The c1 (id) value for lookup int the dataframe passed.\n",
    "\n",
    "    Returns:\n",
    "    int: assignable hits count from the project task id.\n",
    "    \"\"\"\n",
    "    assignable_hits_count = 0\n",
    "    try:\n",
    "        assignable_hits_count = float(df[df.c1 == c1_id].iloc[0][\"c4.project.assignable_hits_count\"])\n",
    "    except (IndexError, TypeError) as err:\n",
    "        print(f\"Cannot calculate task assignable hits count {err}\")\n",
    "\n",
    "    return assignable_hits_count\n",
    "\n",
    "\n",
    "def total_labor_event_count(df: pd.DataFrame, c1_id_start: int, c1_id_end: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns count of events between and including 'TASK_STARTED' to 'FINISHED_TASK' for a task.\n",
    "    (includes working + invisible tasks) \n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk\n",
    "    c1_id_start (int): The task start c1 (id) for 'TASK_STARTED' event time.\n",
    "    c1_id_end (int): The task start c1 (id) for 'FINISHED_TASK' event time.\n",
    "\n",
    "    Returns:\n",
    "    int: Count of task between c1_id_start and c1_id_end (includes working + invisible tasks)\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    try:\n",
    "        count = len(df[df.c1.between(c1_id_start, c1_id_end, inclusive=\"both\") == True])\n",
    "    except TypeError as err:\n",
    "        print(f\"Cannot calculate task count for labor events {err}\")\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def total_labor_duration_in_seconds(dt_start: datetime, dt_end: datetime) -> int:\n",
    "    \"\"\"\n",
    "    Returns duration in seconds between task 'FINISHED_TASK' - 'TASK_STARTED' time.\n",
    "\n",
    "    Parameters:\n",
    "    dt_start (datetime): The task start 'TASK_STARTED' time\n",
    "    dt_end (datetime): The task end 'FINISHED_TASK' time\n",
    "\n",
    "    Returns:\n",
    "    int: Seconds of total time duration.\n",
    "    \"\"\"\n",
    "    duration = 0\n",
    "    try:\n",
    "        duration = (dt_end - dt_start).seconds\n",
    "    except TypeError as err:\n",
    "        print(f\"Cannot calculate labor total time seconds {err}\")\n",
    "\n",
    "    return duration\n",
    "\n",
    "\n",
    "def working_labor_event_count(df: pd.DataFrame, c1_id_start: int, c1_id_end: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns count of 'WORKING' events between and including 'TASK_STARTED' to 'FINISHED_TASK' for a task.\n",
    "    (includes working tasks only)\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk\n",
    "    c1_id_start (int): The task start c1 (id) for 'TASK_STARTED' event time.\n",
    "    c1_id_end (int): The task start c1 (id) for 'FINISHED_TASK' event time.\n",
    "\n",
    "    Returns:\n",
    "    int: Count of task events between c1_id_start and c1_id_end (includes working tasks)\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    try:\n",
    "        # Make sure to sort_values first then reset_index. Otherwise the index could be out of order.\n",
    "        df_working = df[\n",
    "            ((df.c1 >= c1_id_start) & (df.c1 <= c1_id_end)) &\n",
    "            ((df.c9 == 'WORKING') | (df.c9 == 'LOGS'))\n",
    "            ].sort_values(by=['time'], ascending=[True]).reset_index(drop=True)\n",
    "        \n",
    "        count = len(df_working)\n",
    "    except TypeError as err:\n",
    "        print(f\"Cannot calculate task count for labor working events {err}\")\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def working_labor_duration_in_seconds(df: pd.DataFrame, c1_id_start: int, c1_id_end: int, debug=False) -> int:\n",
    "    \"\"\"\n",
    "    Returns duration 'WORKING' events between and including 'TASK_STARTED' to 'FINISHED_TASK' for a task.\n",
    "    (includes working tasks only)\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk\n",
    "    c1_id_start (int): The task start c1 (id) for 'TASK_STARTED' event time.\n",
    "    c1_id_end (int): The task start c1 (id) for 'FINISHED_TASK' event time.\n",
    "\n",
    "    Returns:\n",
    "    int: Seconds of task events between c1_id_start and c1_id_end (includes working tasks)\n",
    "    \"\"\"\n",
    "\n",
    "    if debug:\n",
    "        task_id = df[df[\"c1\"] == c1_id_end].iloc[0][\"c4.task_id\"]\n",
    "        print(f\"\\nTask {task_id}: c1_id_start {c1_id_start}, c1_id_end {c1_id_end}\")\n",
    "\n",
    "    duration = 0\n",
    "    try:\n",
    "        # Make sure to sort_values first then reset_index. Otherwise the index could be out of order.\n",
    "        df_working = df[\n",
    "            ((df.c1 >= c1_id_start) & (df.c1 <= c1_id_end))\n",
    "            ].sort_values(by=['time'], ascending=[True]).reset_index(drop=True)\n",
    "        \n",
    "        for i, j in df_working.iterrows():\n",
    "            if (j[\"c9\"] == 'WORKING') or (j[\"c7\"] == \"ADDED_TASK\" and j[\"c9\"] == 'LOGS'):\n",
    "                try:\n",
    "                    dt_working = datetime.fromisoformat(j[\"time\"])\n",
    "                    dt_next_event = datetime.fromisoformat(df_working.iloc[i + 1][\"time\"])\n",
    "\n",
    "                    if dt_working < dt_next_event:\n",
    "                        duration += (dt_next_event - dt_working).seconds\n",
    "                        if debug:\n",
    "                            print(f\"Difference of id {df_working.iloc[i + 1]['c1']}: ({dt_next_event}) and id {j['c1']}: ({dt_working}) is {(dt_next_event - dt_working).seconds} seconds.\")\n",
    "                    else:\n",
    "                        # Todo: For task_id = \"3M0556243RJC3RXK4Q2QWOJY3G6NF6\" the 'FINISHED_TASK' \n",
    "                        # has earlier datetiem than the 'WORKING' task that comes before it so \n",
    "                        # we need to swap the dates to avoid a negative date and large seconds value.\n",
    "                        duration += (dt_working - dt_next_event).seconds\n",
    "                        if debug:\n",
    "                            print(f\"Difference of id {df_working.iloc[i + 1]['c1']}: ({dt_next_event}) and id {j['c1']}: ({dt_working}) is {(dt_working - dt_next_event).seconds} seconds.\")\n",
    "\n",
    "                except IndexError as err:\n",
    "                    print(f\"Cannot locate next dataframe duration {err}\")\n",
    "            \n",
    "    except (IndexError, TypeError) as err:\n",
    "        print(f\"Cannot calculate task duration for labor working events {err}\")\n",
    "\n",
    "    return duration\n",
    "\n",
    "\n",
    "def adjusted_monetary_reward_in_dollars(\n",
    "        task_monetary_reward_in_dollars: float,\n",
    "        task_estimate_duration_in_seconds: int,\n",
    "        working_labor_duration_in_seconds: int,\n",
    "        invisible_labor_duration_in_seconds: int,\n",
    "\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Returns float indicating adjusted monetary reward in dollars based on parameters passed.\n",
    "\n",
    "    Parameters:\n",
    "    task_monetary_reward_in_dollars (float): requester task monetary reward in dollars.\n",
    "    task_estimate_duration_in_seconds (int): requester task estimate time to complete in seconds.\n",
    "    working_labor_duration_in_seconds (int): working labor duration in seconds.\n",
    "    invisible_labor_duration_in_seconds (int): invisible labor duration in seconds.\n",
    "\n",
    "    Returns:\n",
    "    float: monetary value for reward if we factor in (estimate, working, and invisible parameters passed).\n",
    "    This adjusted value represents actual pay should we include invisible work duration.\n",
    "    \"\"\"\n",
    "    adjusted_pay = 0\n",
    "    try:\n",
    "        # Calculate a pay rate per task event.\n",
    "        task_event_pay_rate = task_monetary_reward_in_dollars / task_estimate_duration_in_seconds\n",
    "\n",
    "        # Using this 'task_event_pay_rate' to build out adjusted pay based on invisible duration.\n",
    "        adjusted_pay = task_monetary_reward_in_dollars + \\\n",
    "                       task_event_pay_rate * (\n",
    "                           working_labor_duration_in_seconds + invisible_labor_duration_in_seconds\n",
    "                           )\n",
    "            \n",
    "    except TypeError as err:\n",
    "        print(f\"Cannot calculate adjusted monetary reward. {err}\")\n",
    "\n",
    "    return adjusted_pay\n",
    "\n",
    "\n",
    "def is_task_completed(df: pd.DataFrame, c1_id_start: int, c1_id_end: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns boolean indicating if there exists an event between c1_id_start to c1_id_end that\n",
    "    has a c9 == 'REJECTED'. If rejected, the return value is 0 meaning that the task was not completed.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Source information from AWS MTurk\n",
    "    c1_id_start (int): The task start c1 (id) for 'TASK_STARTED' event time.\n",
    "    c1_id_end (int): The task start c1 (id) for 'FINISHED_TASK' event time.\n",
    "\n",
    "    Returns:\n",
    "    int: indicates if there is an event between c1_id_start and c1_id_end that includes c9 == 'REJECTED'.\n",
    "    0 means found, 1 means not found.\n",
    "    \"\"\"\n",
    "    completed = 0\n",
    "    try:\n",
    "        # Make sure to sort_values first then reset_index. Otherwise the index could be out of order.\n",
    "        df_working = df[\n",
    "            ((df.c1 >= c1_id_start) & (df.c1 <= c1_id_end)) &\n",
    "            ((df.c9 == 'REJECTED'))\n",
    "            ].sort_values(by=['time'], ascending=[True]).reset_index(drop=True)\n",
    "\n",
    "        if len(df_working) == 0:\n",
    "            completed = 1\n",
    "            \n",
    "    except TypeError as err:\n",
    "        print(f\"Cannot calculate task rejection {err}\")\n",
    "\n",
    "    # If we find a record that's between c1_id_start and c2_id_end with c9 == 'REJECTED' then return False (0).\n",
    "    return completed\n",
    "\n",
    "\n",
    "df_features = pd.DataFrame(columns=[\n",
    "    'platform.name',\n",
    "    'user.id',\n",
    "    'task.id',\n",
    "    'task.monetary_reward_in_dollars',\n",
    "    'task.assignable_hits_count',\n",
    "    'task.requester_name',\n",
    "    'task.estimate_duration_in_seconds',\n",
    "    'total.labor.event_count',\n",
    "    'total.labor.duration_in_seconds',\n",
    "    'working.labor.event_count',\n",
    "    'working.labor.duration_in_seconds',\n",
    "    'invisible.labor.event_count',\n",
    "    'invisible.labor.duration_in_seconds',\n",
    "    'adjusted.monetary_reward_in_dollars',\n",
    "    'completed_task'\n",
    "])\n",
    "\n",
    "# For now grab only the first LIMIT_USERS.\n",
    "if LIMIT_USERS is not None:\n",
    "    unique_users = df_amt['user'].dropna().unique().tolist()[:LIMIT_USERS]\n",
    "else:\n",
    "    # Run for all users.\n",
    "    unique_users = df_amt['user'].dropna().unique().tolist()\n",
    "# unique_users = ['ae862298385abab2a0a1619f8cedef9d']\n",
    "# unique_users = ['149c64b9f9b890bcf32bd2dcf595fd']\n",
    "# '149c64b9f9b890bcf32bd2dcf595fd'\n",
    "# '49e7482b6cda157f388c73b3bcc2ebfc'\n",
    "# 'ae862298385abab2a0a1619f8cedef9d'\n",
    "\n",
    "# Enumerate here to get the index (i) value in case we need to pop(i) values from original list\n",
    "# due to invalid values in the dataset provided.\n",
    "for j, user_id in enumerate(unique_users):\n",
    "\n",
    "    df_amt_user = df_amt[df_amt.user == user_id]\n",
    "\n",
    "    # Use `.tolist()` to convert ndarray to Python list.\n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.ndarray.tolist.html\n",
    "    unique_tasks = df_amt_user['c4.task_id'].dropna().unique().tolist() \n",
    "    # unique_tasks = ['3N5YJ55YXG3AVVNQ3MIU09Y29Q1AN7']\n",
    "    # unique_tasks = ['3CMIQF80GN0XWBC81XEGMJKYJB8Q64']\n",
    "    # [\"3W9XHF7WGLV68SQR2YVUGGPI6QVTK3\"] \n",
    "    # [\"3KA7IJSNW54MTVXHF3TMHNX8OOTPBI\"]\n",
    "    # [\"3E9ZFLPWO0KJDJRNXDHG072XWOZXIK\"]\n",
    "\n",
    "    for i, t in enumerate(unique_tasks):\n",
    "\n",
    "        task_c1_id_start, task_start_date = locate_user_task_start(\n",
    "            df_amt_user, user_id, t, \"https://worker.mturk.com/projects/(.*)/tasks/(.*)?assignment_id(.*)\"\n",
    "            )\n",
    "        task_c1_id_end, task_end_date = locate_user_task_end(\n",
    "            df_amt_user, user_id, t\n",
    "            )\n",
    "        \n",
    "        # If we have record that doesn't have a start time then let's remove it.\n",
    "        # Example: '302U8RURJY0UZS7SBXEJSBSUBN2VNK' doesn't include a 'ADDED_TASK' or 'TASK_STARTED' event.            \n",
    "        if not task_c1_id_start or not task_c1_id_end:\n",
    "            unique_tasks.pop(i)\n",
    "            continue\n",
    "\n",
    "        # Calculate totals for (t = Total, w = Working, i = Invisible)\n",
    "        task_reward_in_dollars = task_monetary_reward_in_dollars(df_amt_user, task_c1_id_end)\n",
    "        task_assign_hits_count = task_assignable_hits_count(df_amt_user, task_c1_id_end)\n",
    "        task_est_duration_in_seconds = task_estimate_duration_in_seconds(df_amt_user, task_c1_id_end)\n",
    "        t_labor_event_count = total_labor_event_count(df_amt_user, task_c1_id_start, task_c1_id_end)\n",
    "        t_labor_duration_in_seconds = total_labor_duration_in_seconds(task_start_date, task_end_date)\n",
    "        w_labor_event_count = working_labor_event_count(df_amt_user, task_c1_id_start, task_c1_id_end)\n",
    "        w_labor_duration_in_seconds = working_labor_duration_in_seconds(df_amt_user, task_c1_id_start, task_c1_id_end, debug=False)\n",
    "        i_labor_event_count = t_labor_event_count - w_labor_event_count\n",
    "        i_labor_duration_in_seconds = t_labor_duration_in_seconds - w_labor_duration_in_seconds\n",
    "\n",
    "        df_features.loc[len(df_features.index)] = [\n",
    "            task_platform_name(df_amt_user, task_c1_id_end),\n",
    "            user_id,\n",
    "            t,\n",
    "            task_reward_in_dollars,\n",
    "            task_assign_hits_count,\n",
    "            task_requester_name(df_amt_user, task_c1_id_end),\n",
    "            task_est_duration_in_seconds,\n",
    "            t_labor_event_count,\n",
    "            t_labor_duration_in_seconds,\n",
    "            w_labor_event_count,\n",
    "            w_labor_duration_in_seconds,\n",
    "            i_labor_event_count,\n",
    "            i_labor_duration_in_seconds,\n",
    "            adjusted_monetary_reward_in_dollars(\n",
    "                task_reward_in_dollars,\n",
    "                task_est_duration_in_seconds,\n",
    "                w_labor_duration_in_seconds,\n",
    "                i_labor_duration_in_seconds\n",
    "            ),\n",
    "            is_task_completed(df_amt_user, task_c1_id_start, task_c1_id_end)\n",
    "        ]\n",
    "\n",
    "# Write out to the final feature *.csv file.\n",
    "df_features.to_csv(f\"../data/cloudworker_tasks.csv\",\n",
    "                encoding='utf-8', header=True, index=False, mode=\"w\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc6300",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
